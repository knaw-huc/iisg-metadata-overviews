{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "\n",
    "Jupyter notebook to query the harvested metadata records from the IISG bibliographic materials (authority)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook makes it possible to get overviews and query the metadata records of the International Institute of Social History (IISG) Bibliographic materials (\"Biblio\"). It uses as source the file \"converted.csv\" obtained via metadata harvesting using the scripts in this repository (https://github.com/lilimelgar/iisg-metadata-overviews).  It contains MARC records from the OAIPMH endpoint. \n",
    "The file contains one record per row, and each marc property (field and subfield) is in a column.\n",
    "\n",
    "Note: the data includes only metadata records at the \"item\" level.\n",
    "\n",
    "Created by Liliana Melgar (April, 2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# A. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Import the required python libraries \n",
    "*(nothing to change)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# to add timestamp to file names\n",
    "import time\n",
    "# import os.path to add paths to files\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Set the path to the csv file \n",
    "*nothing to change if you cloned the repository. If you downloaded the file only (\"biblio_as_csv.gzip\"), then set here the path to where you have downloaded the file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to where the relevant data is located\n",
    "# authority\n",
    "script_dir = os.getcwd()  # Gets the current working directory\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\", \"..\"))  # Moves up two levels to reach 'repo'\n",
    "data_directory_authority = os.path.join(project_root, \"data\", \"authority\")\n",
    "data_converted_authority = os.path.join(data_directory_authority, 'converted')\n",
    "data_downloads_authority = os.path.join(data_directory_authority, 'downloads') #path to the folder where the reports will be downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Read the csv file as a pandas dataframe\n",
    "*nothing to change here, just be patient, IT TAKES LONG TO LOAD (around started at 19.00h and finished sometime before 20:48h same day)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as dataframe\n",
    "authority_df_v0 = pd.read_csv(f'{data_converted_authority}/authority_as_csv_per_field.gzip', sep=\"\\t\", compression='gzip', low_memory=False)\n",
    "# low_memory=False was set after this warning message: \"/var/folders/3y/xbjxw0b94jxg6x2bcbyjsmmcgvnf7q/T/ipykernel_987/2912965462.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Inspect if import was correct\n",
    "Execute the cell and view the general information of the data, which includes the Columns (marc properties with subfields), the Non-Null Count (i.e., how many cells have values; for example: if a cell says \"1 non-null\" it means that only one row has a value); and the Data type (object (i.e., a string or a combination of data types), a float or an integer).\n",
    "\n",
    "- Keep in mind that the MARC labels have 3 characters, and that the fourth character can be an indicator or a subfield. For example: 1000 is Marc label 100 with indicator 0. And 100a is Marc label 100 with subfield a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "authority_df_v0.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Prepare the data for search\n",
    "Because we know that the data doesn't have proper numerical values to be computed, we rather convert all values to strings in order to facilitate querying. This also includes filling in empty values with a standard string: \"null\"\n",
    "*(nothing to change here)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "df_columns = authority_df_v0.columns\n",
    "for column in df_columns:\n",
    "    dataType = authority_df_v0.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        authority_df_v0[column] = authority_df_v0[column].fillna('null')\n",
    "        authority_df_v0[column] = authority_df_v0[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        authority_df_v0[column] = authority_df_v0[column].fillna('null')\n",
    "        authority_df_v0[column] = authority_df_v0[column].astype(str)\n",
    "    if dataType == object:\n",
    "        authority_df_v0[column] = authority_df_v0[column].fillna('null')\n",
    "        authority_df_v0[column] = authority_df_v0[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy\n",
    "authority_df = authority_df_v0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # save the csv (in case one wants to inspect it outside this noteobok). Make sure the \"downloads\" directory exists inside Authority\n",
    "# authority_df.to_csv(f'{data_downloads}/authority_all.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again the general information of the data after having filled in the emtpy values and converted the data types\n",
    "authority_df.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Columns (documentation & labels)\n",
    "Ideally, each field above would have a definition explaining what it means and what kind of values does it contain (in relation to the conventions for creating IISG metadata). That documentation can exist somewhere else (e.g., on Confluence), but this could be a place to start updating or writing those definitions since here one can see the data that they contain in detail.\n",
    "\n",
    "For now, we can consult the MARC21 documentation which explains what each field label means for Authorities: https://www.loc.gov/marc/authority/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARC 21 LABELS FOR AUTHORITY RECORDS\n",
    "\n",
    "# dictionary of Marc labels\n",
    "marc_dictionary = {\"001\": \"Control Number\", \"003\": \"Control Number Identifier\", \"005\": \"Date and Time of Latest Transaction\", \"008\": \"Fixed Length Data Elements\", \"010\": \"Library of Congress Control Number\", \"014\": \"Link to Bibliographic Record for Serial or Multipart Item\", \"016\": \"National Bibliographic Agency Control Number\", \"020\": \"International Standard Book Number\", \"022\": \"International Standard Serial Number\", \"023\": \"Cluster ISSN\", \"024\": \"Other Standard Identifier\", \"031\": \"Musical Incipits Information\", \"034\": \"Coded Cartographic Mathematical Data\", \"035\": \"System Control Number\", \"040\": \"Cataloging Source\", \"042\": \"Authentication Code\", \"043\": \"Geographic Area Code\", \"045\": \"Time Period of Heading\", \"046\": \"Special Coded Dates\", \"050\": \"Library of Congress Call Number\", \"052\": \"Geographic Classification\", \"053\": \"LC Classification Number\", \"055\": \"Library and Archives Canada Call Number\", \"060\": \"National Library of Medicine Call Number\", \"065\": \"Other Classification Number\", \"066\": \"Character Sets Present\", \"070\": \"National Agricultural Library Call Number\", \"072\": \"Subject Category Code\", \"073\": \"Subdivision Usage\", \"075\": \"Type of Entity\", \"080\": \"Universal Decimal Classification Number\", \"082\": \"Dewey Decimal Call Number\", \"083\": \"Dewey Decimal Classification Number\", \"086\": \"Government Document Call Number\", \"087\": \"Government Document Classification Number\", \"09X\": \"Local Call Numbers\", \"100\": \"Heading−Personal Name\", \"110\": \"Heading−Corporate Name\", \"111\": \"Heading−Meeting Name\", \"130\": \"Heading−Uniform Title\", \"147\": \"Heading-Named Event\", \"148\": \"Heading-Chronological Term\", \"150\": \"Heading−Topical Term\", \"151\": \"Heading−Geographic Name\", \"155\": \"Heading−Genre/Form Term\", \"162\": \"Heading−Medium of Performance Term\", \"180\": \"Heading−General Subdivision\", \"181\": \"Heading−Geographic Subdivision\", \"182\": \"Heading−Chronological Subdivision\", \"185\": \"Heading−Form Subdivision\", \"260\": \"Complex See Reference−Subject\", \"335\": \"Extension Plan\", \"336\": \"Content Type\", \"348\": \"Format of Notated Music\", \"360\": \"Complex See Also Reference−Subject\", \"361\": \"Structured Ownership and Custodial History\", \"368\": \"Other Corporate Body Attributes\", \"370\": \"Associated Place\", \"371\": \"Address\", \"372\": \"Field of Activity\", \"373\": \"Associated Group\", \"374\": \"Occupation\", \"375\": \"Gender\", \"376\": \"Family Information\", \"377\": \"Associated Language\", \"378\": \"Fuller Form of Personal Name\", \"380\": \"Form of Work\", \"381\": \"Other Distinguishing Characteristics of Work or Expression\", \"382\": \"Medium of Performance\", \"383\": \"Numeric Designation of Musical Work or Expression\", \"384\": \"Key\", \"385\": \"Audience Characteristics\", \"386\": \"Creator/Contributor Characteristics\", \"387\": \"Representative Expression Characteristics\", \"388\": \"Time Period of Creation\", \"400\": \"See From Tracing−Personal Name\", \"410\": \"See From Tracing−Corporate Name\", \"411\": \"See From Tracing−Meeting Name\", \"430\": \"See From Tracing−Uniform Title\", \"447\": \"See From Tracing-Named Event\", \"448\": \"See From Tracing-Chronological Term\", \"450\": \"See From Tracing−Topical Term\", \"451\": \"See From Tracing−Geographic Name\", \"455\": \"See From Tracing−Genre/Form Term\", \"462\": \"See From Tracing−Medium of Performance Term\", \"480\": \"See From Tracing−General Subdivision\", \"481\": \"See From Tracing−Geographic Subdivision\", \"482\": \"See From Tracing−Chronological Subdivision\", \"485\": \"See From Tracing−Form Subdivision\", \"500\": \"See Also From Tracing−Personal Name\", \"510\": \"See Also From Tracing−Corporate Name\", \"511\": \"See Also From Tracing−Meeting Name\", \"530\": \"See Also From Tracing−Uniform Title\", \"547\": \"See Also From Tracing-Named Event\", \"548\": \"See Also From Tracing-Chronological Term\", \"550\": \"See Also From Tracing−Topical Term\", \"551\": \"See Also From Tracing−Geographic Name\", \"555\": \"See Also From Tracing−Genre/Form Term\", \"562\": \"See Also From Tracing−Medium of Performance Term\", \"580\": \"See Also From Tracing−General Subdivision\", \"581\": \"See Also From Tracing−Geographic Subdivision\", \"582\": \"See Also From Tracing−Chronological Subdivision\", \"585\": \"See Also From Tracing−Form Subdivision\", \"640\": \"Series Dates of Publication and/or Sequential Designation\", \"641\": \"Series Numbering Peculiarities\", \"642\": \"Series Numbering Example\", \"643\": \"Series Place and Publisher/Issuing Body\", \"644\": \"Series Analysis Practice\", \"645\": \"Series Tracing Practice\", \"646\": \"Series Classification Practice\", \"663\": \"Complex See Also Reference−Name\", \"664\": \"Complex See Reference−Name\", \"665\": \"History Reference\", \"666\": \"General Explanatory Reference−Name\", \"667\": \"Nonpublic General Note\", \"670\": \"Source Data Found\", \"672\": \"Title Related to the Entity\", \"673\": \"Title Not Related to the Entity\", \"675\": \"Source Data Not Found\", \"677\": \"Definition\", \"678\": \"Biographical or Historical Data\", \"680\": \"Public General Note\", \"681\": \"Subject Example Tracing Note\", \"682\": \"Deleted Heading Information\", \"688\": \"Application History Note\", \"700\": \"Established Heading Linking Entry−Personal Name\", \"710\": \"Established Heading Linking Entry−Corporate Name\", \"711\": \"Established Heading Linking Entry−Meeting Name\", \"730\": \"Established Heading Linking Entry−Uniform Title\", \"747\": \"Established Heading Linking Entry-Named Event\", \"748\": \"Established Heading Linking Entry-Chronological Term\", \"750\": \"Established Heading Linking Entry−Topical Term\", \"751\": \"Established Heading Linking Entry−Geographic Name\", \"755\": \"Established Heading Linking Entry−Genre/Form Term\", \"762\": \"Established Heading Linking Entry−Medium of Performance Term\", \"780\": \"Subdivision Linking Entry−General Subdivision\", \"781\": \"Subdivision Linking Entry−Geographic Subdivision\", \"782\": \"Subdivision Linking Entry−Chronological Subdivision\", \"785\": \"Subdivision Linking Entry−Form Subdivision\", \"788\": \"Complex Linking Entry Data\", \"856\": \"Electronic Location and Access\", \"857\": \"Electronic Archive Location and Access\", \"880\": \"Alternate Graphic Representation\", \"883\": \"Metadata Provenance\", \"884\": \"Description Conversion Information\", \"885\": \"Matching Information\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Get a glimpse of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## First rows\n",
    "Here you can see a sample of the records, one per line. You can change the value \"10\" to any other desired size for your sample, preferably not too big. You can also use \"tail\" instead of \"head\" to see the records in the last rows.\n",
    "- Keep in mind to scroll horizontally and vertically to see the entire record.\n",
    "- NaN means that the cell is empty.\n",
    "- Arbitrarily, some cells above, we decided that the omega \"Ω\" would be the separator for multi-value cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "authority_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Size (shape) of the data\n",
    "Here you can see how many rows (first value) and how many columns (second value) are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "authority_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Unique values\n",
    "Here you can see a general description of the data, including how many unique values are per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataframe\n",
    "summary_df_v0 = authority_df_v0.describe(include = 'all')\n",
    "summary_df_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df_transposed = summary_df_v0.transpose()\n",
    "summary_df_v1 = summary_df_transposed.drop(columns=['freq'])\n",
    "summary_df = summary_df_v1.rename_axis('marc').reset_index()\n",
    "summary_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df['label'] = summary_df['marc'].map(marc_dictionary)\n",
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.sort_values(by=['unique'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "authority_report_df = summary_df."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Check the values in one column (marc property)\n",
    "At this point you may be curious to know which values are in one column. It may be interesting to observe those that have very few unique values, because they can contain wrong data or wrong columns.\n",
    "- You can change the field inside the quotation marcs for any other field of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authority_df['4J0'].unique().tolist()\n",
    "authority_df['151'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "If you want to see which record contains those values or wrong columns, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check records with non-empty values in a specific column\n",
    "# check_problem1 = authority_df[~authority_df['4J0'].str.contains(\"null\", case=False, regex=True)]\n",
    "check_problem1 = authority_df[authority_df['151'].str.contains('amsterd.*', case=False, regex=True)]\n",
    "check_problem1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Inspect one record\n",
    "If you are interested to see only one record, you can do so by using the recordId (TCN in 001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST (see one record)\n",
    "# check if a string value exists in a column (the string is exactly the same)\n",
    "# test_exact = biblio_df[biblio_df['651a'] == '1362253']\n",
    "# test_exact = authority_df[authority_df['151a'] == 'Srebrenica (Yugoslavia)']\n",
    "# test_exact = authority_df[authority_df['001'] == '239940'] #strikes\n",
    "\n",
    "check_tcn = '327990'\n",
    "check_record = authority_df[authority_df['001'] == check_tcn] #persons (Emma Goldman)\n",
    "check_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_record.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# # choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "# name_file = 'authority_151a_Srebrenica'\n",
    "\n",
    "# test_exact.to_excel(f'{data_downloads}/{name_file}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "# Create a slice/subset with certain column(s)/field(s)\n",
    "At this point you may be curious to know which values are in one column. For example, 040a has 7 unique values, which are those?\n",
    "- You can change the field inside the quotation marcs for any other field of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Test first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = authority_df[['001','151']]\n",
    "test1.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test1[~test1['151'].str.contains(\"null\", case=False, regex=True)]\n",
    "test2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Divide the main df into two dfs (emtpy/non-empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subset with record Id and record of interest, here enter the name of the field(s) that you are interested in separated by commas, each field has to be within single quotation marks, e.g., biblio_df[['001','100e', '110e']]\n",
    "# field_subset_df = biblio_df[['001','090a','901a','245a','245b','260a','852p','852j','866a','902a','leader']] #--> For LA periodicals\n",
    "# field_subset_df = authority_df[['001','150a','151a','155a','leader']] #--> For geographic terms exploration\n",
    "field_subset_df_v0 = authority_df[['001','151','450','550','leader']] #--> For subject thesaurus\n",
    "# field_subset_df_v0 = authority_df[['001','035','100']] #--> For person names\n",
    "\n",
    "# one dataframe for rows with values\n",
    "field_subset_df_v1 = field_subset_df_v0[field_subset_df_v0['151'].str.lower() != 'null'] #to exclude empty values\n",
    "# one dataframe for empty rows\n",
    "field_subset_df_vb = field_subset_df_v0[field_subset_df_v0['151'].str.lower() == 'null'] #to exclude empty values (not used in this case)\n",
    "\n",
    "# field_subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # field_subset_df.info(verbose=True)\n",
    "# field_subset_df.describe()\n",
    "# field_subset_df_vb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2 = field_subset_df_v1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again the number of unique values in your subset\n",
    "field_subset_df_v2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Prepare non-empty subset \n",
    "to have one value per row and one subfield per column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Check separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the longest cell (to get the most problematic as example)\n",
    "# # Convert all cells to string and get their lengths\n",
    "# lengths = field_subset_df_v2.astype(str).map(len)\n",
    "\n",
    "# # Find position (row, col) of the max length\n",
    "# max_row, max_col = lengths.stack().idxmax()\n",
    "\n",
    "# # Get the value from the original DataFrame\n",
    "# longest_cell = field_subset_df_v2.loc[max_row, max_col]\n",
    "\n",
    "# print(f\"Longest cell is in row {max_row}, column '{max_col}' with length {len(str(longest_cell))}\")\n",
    "# print(\"Value:\", longest_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the cell where the separator occurs most frequently (to get the most problematic as example)\n",
    "char = '¶'\n",
    "\n",
    "# Count how many times 'e' appears in each cell (as string)\n",
    "char_counts = field_subset_df_v2.astype(str).map(lambda x: x.count(char))\n",
    "\n",
    "# Find the position of the max count\n",
    "max_row, max_col = char_counts.stack().idxmax()\n",
    "\n",
    "# Get the value from the original DataFrame\n",
    "cell_value = field_subset_df_v2.loc[max_row, max_col]\n",
    "count = char_counts.loc[max_row, max_col]\n",
    "\n",
    "print(f\"The character '{char}' appears most in row {max_row}, column '{max_col}' ({count} times)\")\n",
    "print(\"Cell content:\", cell_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get that record Id using row index number\n",
    "field_subset_df_v2.iloc[208]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the record with the longest value using the TCN (001)\n",
    "# query_value_aprox = field_subset_df_v2[field_subset_df_v2['001'] == '1080191'].copy()\n",
    "# query_value_aprox\n",
    "# test_exact4 = field_subset_df_v2[field_subset_df_v2['001'] == '1466360'] \n",
    "\n",
    "test_exact4 = field_subset_df_v2[field_subset_df_v2['001'] == '177089']\n",
    "test_exact4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Split multi-valued cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into different rows (e.g., explode)\n",
    "\n",
    "# Step 1: Split the column using \"Ω\" as the separator\n",
    "field_subset_df_v2[\"151\"] = field_subset_df_v2[\"151\"].str.split(\"¶\")\n",
    "\n",
    "# Step 2: Explode the list into multiple rows\n",
    "field_subset_df_v2 = field_subset_df_v2.explode(\"151\", ignore_index=True)\n",
    "\n",
    "field_subset_df_v2.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test if the record is split correctly\n",
    "test_exact5 = field_subset_df_v2[field_subset_df_v2['001'] == '177089']\n",
    "test_exact5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape, this should be bigger than the number of records\n",
    "field_subset_df_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v3 = field_subset_df_v2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Split columns per subfield\n",
    "This will make that every subfield goes to its own column, the separator between the subfield label and the value should be checked carefully, I used before \":\" but this caused problems since some values have \":\" in them, thus, now I use also the quotation mark '\":'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE LINES WILL CONVERT FROM THIS FORMAT IN \"DATA\" to a format in which each subfield is in a separate column \n",
    "# with the column name equal to the subfield name, filling in with null the cells where the subfield doesn't exist\n",
    "# data = {\n",
    "#     \"id\": [\"001\", \"002\", \"003\", \"004\"],\n",
    "#     \"values\": ['\"a\":lore;\"b\":ipsum', \n",
    "#                '\"x\":nomine;\"a\":ipsum', \n",
    "#                '\"x\":example;\"c\":dei',\n",
    "#                '\"b\":test;\"y\":test2']\n",
    "# }\n",
    "##########################\n",
    "\n",
    "# # FOR PERSONS (100)\n",
    "# # Parse values into a dictionary-like structure\n",
    "# field_subset_df_v2[\"parsed\"] = field_subset_df_v2[\"100\"].apply(lambda x: {kv.split(\":\")[0]: kv.split(\":\")[1] for kv in x.split(\"⑄\")} if isinstance(x, str) else {})\n",
    "# # Extract all unique keys (column names)\n",
    "# all_keys = sorted(set(k for d in field_subset_df_v2[\"parsed\"] for k in d.keys()))\n",
    "# # all_keys\n",
    "# # Create new columns with values only (remove key names)\n",
    "# for key in all_keys:\n",
    "#     field_subset_df_v2[key] = field_subset_df_v2[\"parsed\"].apply(lambda d: d[key] if key in d else \"null\")\n",
    "# # df\n",
    "# # Keep only relevant columns\n",
    "# field_subset_df_v3 = field_subset_df_v2[[\"001\", \"leader\"] + all_keys]\n",
    "\n",
    "####\n",
    "# FOR SUBJECTS (650)\n",
    "# Parse values into a dictionary-like structure\n",
    "field_subset_df_v3[\"parsed\"] = field_subset_df_v3[\"151\"].apply(lambda x: {kv.split('\":')[0]: kv.split('\":')[1] for kv in x.split(\"⑄\")} if isinstance(x, str) else {})\n",
    "# Extract all unique keys (column names)\n",
    "all_keys = sorted(set(k for d in field_subset_df_v3[\"parsed\"] for k in d.keys()))\n",
    "# all_keys\n",
    "# Create new columns with values only (remove key names)\n",
    "for key in all_keys:\n",
    "    field_subset_df_v3[key] = field_subset_df_v3[\"parsed\"].apply(lambda d: d[key] if key in d else \"null\")\n",
    "# df\n",
    "# Keep only relevant columns\n",
    "field_subset_df_v4 = field_subset_df_v3[[\"001\", \"151\", \"450\", \"550\", \"leader\"] + all_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v4.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v5 = field_subset_df_v4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "If the split shows some strange columns, you may need to inspect if it went well, for that it may be useful to check the unique values in the suspicious column, but since column names contain quotation marks, it is best to rename them first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAME COLUMNS\n",
    "df_columns = field_subset_df_v5.columns\n",
    "# df_columns\n",
    "field_subset_df_v5.columns = field_subset_df_v5.columns.str.replace('\"', '', regex=False).str.strip()\n",
    "field_subset_df_v5.rename(columns={'151': '151', 'leader': 'leader', '450': '450', '550': '550', '1': 'indicator_1', 'a': 'subfield_a'}, inplace=True)\n",
    "field_subset_df_v5.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "Now you can inspect the column's unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "listmarc151 = field_subset_df_v5['151'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(listmarc151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v8 = field_subset_df_v5.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_richard2 = field_subset_df_v8[['001','subfield_a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# # choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "# # name_file = 'biblio_author_person_field_100a' #--> authors test\n",
    "# # name_file = 'biblio_geo_651a' #--> geoterms\n",
    "# # name_file = 'authorities_geo_151a_parenthesis'\n",
    "# # name_file = 'subject_terms_per_150'\n",
    "# name_file = 'person_names_per_100'\n",
    "name_file = 'subject_terms_Authority_151a'\n",
    "\n",
    "# # field_subset_df.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "# ## or download to csv\n",
    "report_richard2.to_csv(f'{data_downloads_authority}/{name_file}.csv', index=False) # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Report problematic records (now drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see code in biblio_query, for now nothing to report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "## Concatenate the dataframes again (empty/non-empty) -if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # special for when there were two separated dfs one with non-empty values one with empty values, this is useful to generate\n",
    "# # report of empty fields, especially for the pie chart with records that don't have any value in 650a\n",
    "# field_subset_df_v8 = field_subset_df_v5.reset_index(drop=True)\n",
    "# # create column 650 with 'null/notnull' values to be able to filter\n",
    "# field_subset_df_v8[\"150\"] = 'notnull'\n",
    "# frames = [field_subset_df_v8, field_subset_df_vb]\n",
    "# field_subset_df_v9 = pd.concat(frames, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if it went well\n",
    "# field_subset_df_v9['150'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# field_subset_df_v9.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert datatypes and fill in empty values\n",
    "# df_columns = field_subset_df_v9.columns\n",
    "# for column in df_columns:\n",
    "#     dataType = field_subset_df_v9.dtypes[column]\n",
    "#     if dataType == np.float64:\n",
    "#         field_subset_df_v9[column] = field_subset_df_v9[column].fillna('null')\n",
    "#         field_subset_df_v9[column] = field_subset_df_v9[column].astype(str)\n",
    "#     if dataType == np.int_:\n",
    "#         biblio_field_subset_df_v9df_v0[column] = field_subset_df_v9[column].fillna('null')\n",
    "#         field_subset_df_v9[column] = field_subset_df_v9[column].astype(str)\n",
    "#     if dataType == object:\n",
    "#         field_subset_df_v9[column] = field_subset_df_v9[column].fillna('null')\n",
    "#         field_subset_df_v9[column] = field_subset_df_v9[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# field_subset_df_v10 = field_subset_df_v9.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "## Other operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many fields 450 are filled in\n",
    "with450 = field_subset_df_v8[~field_subset_df_v8['550'].str.contains(\"null\", case=False, regex=True)]\n",
    "with450.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "## Download slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df = field_subset_df_v8.reset_index(drop=True)\n",
    "field_subset_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "# name_file = 'biblio_author_person_field_100a' #--> authors test\n",
    "# name_file = 'biblio_geo_651a' #--> geoterms\n",
    "# name_file = 'biblio_serials_report_simon' #--> serials report for Simon 2024-11-14\n",
    "name_file = 'subjects_authority_150_subfields' # for thesaurus report (202504? and 20250414)\n",
    "# name_file = 'persons_100_subfields' # for thesaurus report (20250414)\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# field_subset_df.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "## or download to csv\n",
    "field_subset_df.to_csv(f'{data_downloads_authority}/{name_file}_{timestr}.csv', index=False) # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "# Create a subset of records with a certain value in a given column (toDo)\n",
    "You may also want to create a list of the records with a certain value in a given column, for example, for field 100e you got these unique values: ['creator.', 'null', 'creator']. You may want to get only the list of records that have \"creator.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when the file above is too big, it's useful sometimes to download it and upload it here again\n",
    "path = '/Users/lilianam/workspace/iisg-metadata-overviews/biblio/data'\n",
    "field_subset_df = pd.read_csv(f'{path}/biblio_titles.csv.gz', sep=\",\", compression='gzip', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a string value exists in a column (the string is exactly the same)\n",
    "# query_value_exact = field_subset_df[field_subset_df['100a'] == 'Hajnal, Henri.'] --> I used in ....\n",
    "\n",
    "query_value_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a string value exists in a column (the string is approximately the same)\n",
    "# you may want to find the records that have either \"creator.\" (with dot) or \"creator\" without dot, but not the null values\n",
    "# here it's possible to use regular expressions\n",
    "\n",
    "query_value_aprox = field_subset_df[field_subset_df['100a'].str.contains(\"Ka.*nelson, Berl\", case=False, regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_value_aprox.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some idea of how many rows are in this set\n",
    "query_value_aprox.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again the number of unique values in your subset\n",
    "query_value_aprox.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "# name_file = 'biblio_author_person_field_100a_henri'\n",
    "name_file = 'biblio_to_map_la_periodicals_852j'\n",
    "\n",
    "query_value_aprox.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "## or download to csv\n",
    "# query_value_aprox.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "# Create subsets using inverse query (toDo)\n",
    "You may need to create a report with all the records that do not contain a certain value. For example, because we used \"null\" to fill in all empty values, one could create a list with all the records that have a value in a certain column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a slice with the records that have non-null values in the column of interest\n",
    "# Note: if you want to query the subset instead of the whole data, then replace \"biblio_df\" with \"field_subset_df\" and run the cell again\n",
    "\n",
    "query_inverse = biblio_df[~biblio_df['100a'].str.contains(\"null\", case=False, regex=True)]\n",
    "\n",
    "query_inverse.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some info about the subset you got as a result of the query:\n",
    "query_inverse.info(verbose=True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "name_file = 'biblio_author_person_field_100a_notEmpty'\n",
    "\n",
    "query_inverse.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "## or download to csv\n",
    "# query_inverse.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "# Query for a specific record (toDo)\n",
    "You may want to see the details of a specific record, this can be done in two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. by using the index position. Example: This item: ToDo has index position 0. \n",
    "# This position can be seen in the left corner of the entire table (cell above in Section5: biblio_df.head(10))\n",
    "# We will query it using the entire version of the data, not the subset\n",
    "\n",
    "# show record vertically using index position\n",
    "query_recordIndex = biblio_df.iloc[0]\n",
    "query_recordIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. By using the record Id using the Marc field 001\n",
    "query_recordId = biblio_df[biblio_df['001'] == '8']\n",
    "query_recordId"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
