{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a75338e",
   "metadata": {},
   "source": [
    "# Jupyter notebook to query the harvested metadata records from the IISG bibliographic materials (biblio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c03f4a",
   "metadata": {},
   "source": [
    "This notebook makes it possible to get overviews and query the metadata records of the International Institute of Social History (IISG) Bibliographic materials (\"Biblio\"). It uses as source the file \"converted.csv\" obtained via metadata harvesting using the scripts in this repository (https://github.com/lilimelgar/iisg-metadata-overviews).  It contains MARC records from the OAIPMH endpoint. \n",
    "The file contains one record per row, and each marc property (field and subfield) is in a column.\n",
    "\n",
    "Note: the data includes only metadata records at the \"item\" level.\n",
    "\n",
    "Created by Liliana Melgar (April, 2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7522543",
   "metadata": {},
   "source": [
    "# A. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64cb568",
   "metadata": {},
   "source": [
    "## A1. Import the required python libraries \n",
    "*(nothing to change)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e78d4-d5ae-4d82-b2a7-99968026fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pymarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "344696ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "from pymarc import MARCReader\n",
    "from pymarc import exceptions as exc\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# to add timestamp to file names\n",
    "import time\n",
    "# import os.path to add paths to files\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81a8db4",
   "metadata": {},
   "source": [
    "## A2. Set the path to the csv file \n",
    "*nothing to change if you cloned the repository. If you downloaded the file only (\"biblio_as_csv.gzip\"), then set here the path to where you have downloaded the file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b5dfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lilianam/workspace/iisg-metadata-overviews/data/biblio/extracted/0/0/0/0\n"
     ]
    }
   ],
   "source": [
    "# path to where the relevant data is located\n",
    "# biblio\n",
    "\n",
    "test_folder = 'extracted/0/0/0/0'\n",
    "\n",
    "script_dir = os.getcwd()  # Gets the current working directory\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\", \"..\"))  # Moves up two levels to reach 'repo'\n",
    "data_directory_biblio = os.path.join(project_root, \"data\", \"biblio\", test_folder)\n",
    "print(data_directory_biblio)\n",
    "# data_converted_biblio = os.path.join(data_directory_biblio, 'converted')\n",
    "# data_downloads_biblio = os.path.join(data_directory_biblio, 'downloads') #path to the folder where the reports will be downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97cde0ec-de33-489c-b478-9ea32e56c0be",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/Users/lilianam/workspace/iisg-metadata-overviews/data/biblio/extracted/0/0/0/0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(data_directory_biblio, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[1;32m      2\u001b[0m     reader \u001b[38;5;241m=\u001b[39m MARCReader(fh)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m record \u001b[38;5;129;01min\u001b[39;00m reader:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/Users/lilianam/workspace/iisg-metadata-overviews/data/biblio/extracted/0/0/0/0'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(data_directory_biblio, 'rb') as fh:\n",
    "    reader = MARCReader(fh)\n",
    "    for record in reader:\n",
    "        if record:\n",
    "            # consume the record:\n",
    "            print(record.title)\n",
    "        elif isinstance(reader.current_exception, exc.FatalReaderError):\n",
    "            # data file format error\n",
    "            # reader will raise StopIteration\n",
    "            print(reader.current_exception)\n",
    "            print(reader.current_chunk)\n",
    "        else:\n",
    "            # fix the record data, skip or stop reading:\n",
    "            print(reader.current_exception)\n",
    "            print(reader.current_chunk)\n",
    "            # break/continue/raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860a25a-1308-4f81-9a6b-52c1a280f700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680197f-65ec-4a46-9069-c5352b4fc798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157902f-ea1e-4d5b-845c-aa1798a81085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f79da8e",
   "metadata": {},
   "source": [
    "## A3. Read the csv file as a pandas dataframe\n",
    "*nothing to change here, just be patient, IT TAKES LONG TO LOAD (around started at 19.00h and finished sometime before 20:48h same day)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde84496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as dataframe\n",
    "# biblio_df_v0 = pd.read_csv(f'{data_converted_biblio}/biblio_as_csv_per_field.gzip', sep=\"\\t\", compression='gzip', low_memory=False)\n",
    "# low_memory=False was set after this warning message: \"/var/folders/3y/xbjxw0b94jxg6x2bcbyjsmmcgvnf7q/T/ipykernel_987/2912965462.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3a194d",
   "metadata": {},
   "source": [
    "# B. First overview and data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f117ec8a",
   "metadata": {},
   "source": [
    "## B1. First overview: all fields and data types\n",
    "Execute the cell and view the general information of the data, which includes the Columns (marc properties with subfields), the Non-Null Count (i.e., how many cells have values; for example: if a cell says \"1 non-null\" it means that only one row has a value); and the Data type (object (i.e., a string or a combination of data types), a float or an integer).\n",
    "- Keep in mind that the MARC labels have 3 characters, and that the fourth character can be an indicator or a subfield. For example: 1000 is Marc label 100 with indicator 0. And 100a is Marc label 100 with subfield a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143e8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_df_v0.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f729a27-9247-48e0-b21b-4dc9ba17250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test100 = biblio_df_v0[['001','650']]\n",
    "# test100 = biblio_df_v0[['001','100']]\n",
    "test100.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80219c9-4a21-427b-9db9-929fcca53348",
   "metadata": {},
   "outputs": [],
   "source": [
    "test100.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa5f9b0",
   "metadata": {},
   "source": [
    "## B2. Optional (documentation)\n",
    "Ideally, each field above would have a definition explaining what it means and what kind of values does it contain (in relation to the conventions for creating IISG metadata). That documentation can exist somewhere else (e.g., on Confluence), but this could be a place to start updating or writing those definitions since here one can see the data that they contain in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71f685",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a169b5e",
   "metadata": {},
   "source": [
    "## B3. Prepare the data for search\n",
    "Because we know that the data doesn't have proper numerical values to be computed, we rather convert all values to strings in order to facilitate querying. This also includes filling in empty values with a standard string: \"null\"\n",
    "*(nothing to change here)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c5410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "df_columns = biblio_df_v0.columns\n",
    "for column in df_columns:\n",
    "    dataType = biblio_df_v0.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].fillna('null')\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].fillna('null')\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].astype(str)\n",
    "    if dataType == object:\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].fillna('null')\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f53c768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy\n",
    "biblio_df = biblio_df_v0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f53993-5721-431d-80a8-e394414e0cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the csv\n",
    "# biblio_df.to_csv('biblio_all.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e92de00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again the general information of the data after having filled in the emtpy values and converted the data types\n",
    "biblio_df.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010f8e0b",
   "metadata": {},
   "source": [
    "# C. Get a glimpse of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a598cdf",
   "metadata": {},
   "source": [
    "## C1. First rows\n",
    "Here you can see a sample of the records, one per line. You can change the value \"10\" to any other desired size for your sample, preferably not too big. You can also use \"tail\" instead of \"head\" to see the records in the last rows.\n",
    "- Keep in mind to scroll horizontally and vertically to see the entire record.\n",
    "- NaN means that the cell is empty.\n",
    "- Arbitrarily, some cells above, we decided that the omega \"Ω\" would be the separator for multi-value cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0638495f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biblio_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe52e6",
   "metadata": {},
   "source": [
    "## C2. Size (shape) of the data\n",
    "Here you can see how many rows (first value) and how many columns (second value) are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821f442",
   "metadata": {},
   "source": [
    "## C3. Unique values\n",
    "Here you can see a general description of the data, including how many unique values are per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b95bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataframe\n",
    "biblio_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef481ca2-472f-427f-88bb-0cbb20cea6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "biblio_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade61af7-46a8-43c3-9e68-3be5e06c9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblio_df['100'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045596c-9776-4d13-a786-427a5b4d380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name_file = 'biblio_all_harvested20241117_compr'\n",
    "# biblio_df.to_csv(f'{data_downloads}/{name_file}.csv', index=False, compression='gzip') # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38a3cbd",
   "metadata": {},
   "source": [
    "# D. Check the values in one column (marc property)\n",
    "At this point you may be curious to know which values are in one column. For example, 100e has only 3 unique values, which are those?\n",
    "- You can change the field inside the quotation marcs for any other field of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a076f9d-a6b1-4dbb-8948-bb6faa1b0a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ebd07a-e37e-4d93-a6d0-a77458c73d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST (see one record)\n",
    "# # check if a string value exists in a column (the string is exactly the same)\n",
    "# # test_exact = biblio_df[biblio_df['651a'] == '1362253']\n",
    "# # test_exact = biblio_df[biblio_df['651a'] == 'Srebrenica (Yugoslavia)']\n",
    "\n",
    "# test_exact = biblio_df[biblio_df['001'] == '1466360'] \n",
    "test_exact = biblio_df[biblio_df['001'] == '1534711']\n",
    "test_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669c672-b946-4f57-baf4-f347369ea87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# # choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "# name_file = 'biblio_651a_Srebrenica'\n",
    "\n",
    "# test_exact.to_excel(f'{data_downloads}/{name_file}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dc7074-ea8a-458b-bb60-f53bb596c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_exact.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblio_df['100a'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810a664-2079-4144-b830-af721c283f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25de1c14",
   "metadata": {},
   "source": [
    "## D1. Create a subset with certain column(s)/field(s)\n",
    "At this point you may have thought that you could perhaps correct some of the records which contain an inconsistent value. For example, in the first version of this data, if you queried above for \"biblio_df['100e'].unique()\" you may have obtained certain values. You may decide that you want to change one or some of them into another value. But for this, you need the TCN (record Id) numbers. The command below facilitates creating a subset with the TCN and the field of interest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create subset with record Id and record of interest, here enter the name of the field(s) that you are interested in separated by commas, each field has to be within single quotation marks, e.g., biblio_df[['001','100e', '110e']]\n",
    "# # field_subset_df = biblio_df[['001','090a','901a','245a','245b','260a','852p','852j','866a','902a','leader']] #--> For LA periodicals\n",
    "# # field_subset_df = biblio_df[['001','245a','245b','6510','651a','695g','leader']] #--> For geographic terms exploration\n",
    "# # field_subset_df = biblio_df[['001','090a','901a','245a','245b','260a','852a','852b','852c','852j','852n','852p','866a','902a','leader']] #--> For periodicals Simon report\n",
    "\n",
    "## CREATE SUBSET FOR SUBJECT TERMS ANALYSIS (columns starting with 6)\n",
    "# df_columns = biblio_df.columns\n",
    "# columns_subjects = []\n",
    "# for column in biblio_df.columns:\n",
    "#     if column.startswith(\"6\"):\n",
    "#         columns_subjects.append(column)\n",
    "# field_subset_df = biblio_df[['001', 'leader'] + columns_subjects] #--> For av thesaurus terms\n",
    "\n",
    "# CREATE A SUBSET WITH \"650\" ONLY to evaluate subject terms\n",
    "field_subset_df_v0 = biblio_df[['001', 'leader','650']] #--> For av thesaurus terms\n",
    "# one dataframe for rows with values\n",
    "field_subset_df_v1 = field_subset_df_v0[field_subset_df_v0['650'].str.lower() != 'null'] #to exclude empty values\n",
    "# one dataframe for empty rows\n",
    "field_subset_df_vb = field_subset_df_v0[field_subset_df_v0['650'].str.lower() == 'null'] #to exclude empty values (not used in this case)\n",
    "\n",
    "# # CREATE A SUBSET WITH \"100\" ONLY to evaluate persons\n",
    "# field_subset_df_v0 = biblio_df[['001', 'leader','100']] #--> For persons\n",
    "# # one dataframe for rows with values\n",
    "# field_subset_df_v1 = field_subset_df_v0[field_subset_df_v0['100'].str.lower() != 'null'] #to exclude empty values\n",
    "# # one dataframe for empty rows\n",
    "# field_subset_df_vb = field_subset_df_v0[field_subset_df_v0['100'].str.lower() == 'null'] #to exclude empty values (not used in this case)\n",
    "\n",
    "# field_subset_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938506e-0498-4284-a279-a000b774d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # field_subset_df.info(verbose=True)\n",
    "# field_subset_df.describe()\n",
    "# field_subset_df_vb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef30934e-30f5-49f2-8ac7-eb8581acb62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2 = field_subset_df_v1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80278ba-fec7-4670-aa24-876a33a293da",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cb8962-9328-49ac-84b5-12a1a5159506",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4d669-c30f-4484-8172-bf4049db8f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baddaf82-5dea-483e-ab2e-5c1ef24e5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the longest cell (to get the most problematic as example)\n",
    "# # Convert all cells to string and get their lengths\n",
    "# lengths = field_subset_df_v2.astype(str).map(len)\n",
    "\n",
    "# # Find position (row, col) of the max length\n",
    "# max_row, max_col = lengths.stack().idxmax()\n",
    "\n",
    "# # Get the value from the original DataFrame\n",
    "# longest_cell = field_subset_df_v2.loc[max_row, max_col]\n",
    "\n",
    "# print(f\"Longest cell is in row {max_row}, column '{max_col}' with length {len(str(longest_cell))}\")\n",
    "# print(\"Value:\", longest_cell)\n",
    "\n",
    "# Get the cell where the separator occurs most frequently (to get the most problematic as example)\n",
    "char = '¶'\n",
    "\n",
    "# Count how many times 'e' appears in each cell (as string)\n",
    "char_counts = field_subset_df_v2.astype(str).map(lambda x: x.count(char))\n",
    "\n",
    "# Find the position of the max count\n",
    "max_row, max_col = char_counts.stack().idxmax()\n",
    "\n",
    "# Get the value from the original DataFrame\n",
    "cell_value = field_subset_df_v2.loc[max_row, max_col]\n",
    "count = char_counts.loc[max_row, max_col]\n",
    "\n",
    "print(f\"The character '{char}' appears most in row {max_row}, column '{max_col}' ({count} times)\")\n",
    "print(\"Cell content:\", cell_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c5fa0-30b8-4163-ab40-9857bc5720ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2.iloc[182696]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af2577-3b9a-4885-9f72-18e9c9a4d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_value_aprox = field_subset_df_v2[field_subset_df_v2['001'] == '1021749'].copy()\n",
    "query_value_aprox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a384983-f81a-4daa-8bae-87707003e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into different rows (e.g., explode)\n",
    "\n",
    "# Step 1: Split the column using \"Ω\" as the separator\n",
    "# field_subset_df_v2[\"100\"] = field_subset_df_v2[\"100\"].str.split(\"¶\")\n",
    "field_subset_df_v2[\"650\"] = field_subset_df_v2[\"650\"].str.split(\"¶\")\n",
    "\n",
    "# Step 2: Explode the list into multiple rows\n",
    "# field_subset_df_v2 = field_subset_df_v2.explode(\"100\", ignore_index=True)\n",
    "field_subset_df_v2 = field_subset_df_v2.explode(\"650\", ignore_index=True)\n",
    "\n",
    "field_subset_df_v2.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52840c-74f8-4590-915b-3b8e04da7f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_exact4 = field_subset_df_v2[field_subset_df_v2['001'] == '1466360'] \n",
    "test_exact4 = field_subset_df_v2[field_subset_df_v2['001'] == '1534711']\n",
    "test_exact4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f72898-4321-4892-9172-407c1d9ca901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE LINES WILL CONVERT FROM THIS FORMAT IN \"DATA\" to a format in which each subfield is in a separate column \n",
    "# with the column name equal to the subfield name, filling in with null the cells where the subfield doesn't exist\n",
    "# data = {\n",
    "#     \"id\": [\"001\", \"002\", \"003\", \"004\"],\n",
    "#     \"values\": ['\"a\":lore;\"b\":ipsum', \n",
    "#                '\"x\":nomine;\"a\":ipsum', \n",
    "#                '\"x\":example;\"c\":dei',\n",
    "#                '\"b\":test;\"y\":test2']\n",
    "# }\n",
    "##########################\n",
    "\n",
    "# # FOR PERSONS (100)\n",
    "# # Parse values into a dictionary-like structure\n",
    "# field_subset_df_v2[\"parsed\"] = field_subset_df_v2[\"100\"].apply(lambda x: {kv.split(\":\")[0]: kv.split(\":\")[1] for kv in x.split(\"⑄\")} if isinstance(x, str) else {})\n",
    "# # Extract all unique keys (column names)\n",
    "# all_keys = sorted(set(k for d in field_subset_df_v2[\"parsed\"] for k in d.keys()))\n",
    "# # all_keys\n",
    "# # Create new columns with values only (remove key names)\n",
    "# for key in all_keys:\n",
    "#     field_subset_df_v2[key] = field_subset_df_v2[\"parsed\"].apply(lambda d: d[key] if key in d else \"null\")\n",
    "# # df\n",
    "# # Keep only relevant columns\n",
    "# field_subset_df_v3 = field_subset_df_v2[[\"001\", \"leader\"] + all_keys]\n",
    "\n",
    "####\n",
    "# FOR SUBJECTS (650)\n",
    "# Parse values into a dictionary-like structure\n",
    "field_subset_df_v2[\"parsed\"] = field_subset_df_v2[\"650\"].apply(lambda x: {kv.split('\":')[0]: kv.split('\":')[1] for kv in x.split(\"⑄\")} if isinstance(x, str) else {})\n",
    "# Extract all unique keys (column names)\n",
    "all_keys = sorted(set(k for d in field_subset_df_v2[\"parsed\"] for k in d.keys()))\n",
    "# all_keys\n",
    "# Create new columns with values only (remove key names)\n",
    "for key in all_keys:\n",
    "    field_subset_df_v2[key] = field_subset_df_v2[\"parsed\"].apply(lambda d: d[key] if key in d else \"null\")\n",
    "# df\n",
    "# Keep only relevant columns\n",
    "field_subset_df_v3 = field_subset_df_v2[[\"650\", \"leader\"] + all_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190fd508-c2c1-4fc0-8569-5be1ab92a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d15056-f40d-48da-bd7b-350cd17ff658",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85f92b-6482-4f18-9642-3ebe1fda6a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6491d72-e846-48bb-91f4-70310dc9b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAME COLUMNS\n",
    "df_columns = field_subset_df_v3.columns\n",
    "# df_columns\n",
    "field_subset_df_v3.rename(index={0: \"650\", 1: \"leader\", 2: \"empty\", 3: \"indicator_0\", 4: \"indicator_1\", 5: \"indicator_2\", 6: \"indicator_4\", 7: \"indicator_6\", 8: \"indicator_8\", 9: \"subfield_a\", 10: \"subfield_d\", 11: \"subfield_g\", 12: \"subfield_l\", 13: \"subfield_v\", 14: \"subfield_x\", 15: \"subfield_y\", 16: \"subfield_z\"}).copy()\n",
    "test = field_subset_df_v3.copy()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854fe9a-5043-40c6-80dc-c2ebe3da8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check what it's in the column with no value \" \"\n",
    "check = field_subset_df_v3[['001','\" \"']]\n",
    "# check_problem.describe()\n",
    "\n",
    "check_problem = check[~check['\" \"'].str.contains(\"null\", case=False, regex=True)]\n",
    "\n",
    "check_problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39abb7a-ed85-4cee-bdb1-4eb7e5bfa936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f83d61-f70f-4316-ba52-c0f3e13bb109",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_exact5 = field_subset_df_v3[field_subset_df_v3['001'] == '1466360']\n",
    "test_exact5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94915004-d637-462e-93af-c0a38cc26bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special for when there were two separated dfs one with non-empty values one with empty values, this is useful to generate\n",
    "# report of empty fields, especially for the pie chart with records that don't have any value in 650a\n",
    "field_subset_df_v4 = field_subset_df_v3.reset_index(drop=True)\n",
    "field_subset_df_v4[\"100\"] = 'notnull'\n",
    "frames = [field_subset_df_v4, field_subset_df_vb]\n",
    "field_subset_df_v5 = pd.concat(frames, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185f6208-e7f3-4b87-8349-605c8915f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v5.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0b57a-5280-478f-9cfc-d13ee7edad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the leader code to be able to generate report per item type\n",
    "# split using character position, remember the leader is at position 6 to 8\n",
    "field_subset_df_v5[\"leader_code\"] = field_subset_df_v5[\"leader\"].map(lambda x: x[5:8])\n",
    "field_subset_df_v5.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a62df8-34b8-4a89-84d8-c3ff85288dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df = field_subset_df_v5.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6314daf-38d4-4213-b1f4-3abef25050d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709720a5-bd58-4bc7-be9f-65ac3b94214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df['\"0\"'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2010db-6863-4ce1-9efc-44221f9deeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df['\"0\"'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfab58-91af-44f3-a56d-976191e7fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df['\"0\"'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb349422-3cc7-4c51-8f60-8d7acb88deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df['\"a\"'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "# name_file = 'biblio_author_person_field_100a' #--> authors test\n",
    "# name_file = 'biblio_geo_651a' #--> geoterms\n",
    "# name_file = 'biblio_serials_report_simon' #--> serials report for Simon 2024-11-14\n",
    "# name_file = 'subjects_600_subfields' # for thesaurus report (202504? and 20250414)\n",
    "name_file = 'persons_100_subfields' # for thesaurus report (20250414)\n",
    "\n",
    "\n",
    "# field_subset_df.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "## or download to csv\n",
    "field_subset_df.to_csv(f'{data_downloads_biblio}/{name_file}.csv', index=False) # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7dbb6c-1450-4479-a50c-a30bdc573efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2cc919-d1f0-4211-9f16-2f642cee4c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7b6dc-137b-452b-92da-f9d8bd43ccd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2a66ab-7060-49f2-ba14-a2b6e88cf177",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST SQL CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825296e-cab1-4451-9be0-9bd5b255d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "\n",
    "field_subset_df.to_sql(name='testdb', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993a914-c05d-46c1-bc22-7ea5cb5ee0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"SELECT * FROM testdb\")).fetchall()\n",
    "    [(0, '001'), (1, '245a'), (2, '245b')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a479d-1160-4209-9e72-7f72b7062138",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"PRAGMA table_info(testdb)\")).fetchall()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ee23c-faad-47ab-ba0d-4afc5a3a71e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST SQL ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e9ed3",
   "metadata": {},
   "source": [
    "## D2. Create a subset of records with a certain value in a given column\n",
    "You may also want to create a list of the records with a certain value in a given column, for example, for field 100e you got these unique values: ['creator.', 'null', 'creator']. You may want to get only the list of records that have \"creator.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9734d-54a8-4be2-adda-3318dda7030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when the file above is too big, it's useful sometimes to download it and upload it here again\n",
    "path = '/Users/lilianam/workspace/iisg-metadata-overviews/biblio/data/downloads'\n",
    "# field_subset_df = pd.read_csv(f'{path}/biblio_titles.csv.gz', sep=\",\", compression='gzip', low_memory=False)\n",
    "\n",
    "field_subset_df = pd.read_csv(f'{path}/biblio_serials_report_simon.csv', sep=\",\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dfba0-00aa-4fec-9026-db8424737f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9534ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if a string value exists in a column (the string is exactly the same)\n",
    "# query_value_exact = field_subset_df[field_subset_df['100a'] == 'Hajnal, Henri.']\n",
    "# query_value_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8edfcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a string value exists in a column (the string is approximately the same)\n",
    "# you may want to find the records that have either \"creator.\" (with dot) or \"creator\" without dot, but not the null values\n",
    "# here it's possible to use regular expressions\n",
    "\n",
    "# query_value_aprox = field_subset_df[field_subset_df['852j'].str.contains(\"ZDF|ZF|ZDK|ZO|XZK|ZDO|ZK\", case=True, regex=True)] # for LA periodicals\n",
    "\n",
    "# query_value_aprox2 = (field_subset_df[field_subset_df['leader'].str.contains('cas|nas') & field_subset_df['852c'].str.contains('NIBG')]) --> Simon report, not good\n",
    "\n",
    "query_value_aprox3 = field_subset_df[field_subset_df['leader'].str.contains(\"cas|nas\", case=True, regex=True)] # for simon report v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f7dd8-521f-4bcc-bc3f-085f6b1b89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_value_aprox3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some idea of how many rows are in this set\n",
    "query_value_aprox3.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436afc78-2f16-4be5-ae7e-d8c8b3933c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "serials_subset = query_value_aprox3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fe01a-c9cd-47aa-9a7b-4da72b1dbf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "df_columns = serials_subset.columns\n",
    "for column in df_columns:\n",
    "    dataType = serials_subset.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        serials_subset[column] = serials_subset[column].fillna('null')\n",
    "        serials_subset[column] = serials_subset[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        serials_subset[column] = serials_subset[column].fillna('null')\n",
    "        serials_subset[column] = serials_subset[column].astype(str)\n",
    "    if dataType == object:\n",
    "        serials_subset[column] = serials_subset[column].fillna('null')\n",
    "        serials_subset[column] = serials_subset[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bfb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again the number of unique values in your subset\n",
    "serials_subset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c6c73-d1bb-4a4b-8545-35138482d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "serials_subset.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e2070-d39e-4b2a-9a9d-f7a2a3e757be",
   "metadata": {},
   "outputs": [],
   "source": [
    "serials_subset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43026b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "# name_file = 'biblio_author_person_field_100a_henri'\n",
    "# name_file = 'biblio_to_map_la_periodicals_852j'\n",
    "\n",
    "name_file = 'biblio_serials_simon_report_v4'\n",
    "\n",
    "# serials_subset.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "## or download to csv\n",
    "# query_value_aprox.to_csv()\n",
    "serials_subset.to_csv(f'{data_downloads}/{name_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c72f9d",
   "metadata": {},
   "source": [
    "# E. Create subsets using inverse query\n",
    "You may need to create a report with all the records that do not contain a certain value. For example, because we used \"null\" to fill in all empty values, one could create a list with all the records that have a value in a certain column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a slice with the records that have non-null values in the column of interest\n",
    "# Note: if you want to query the subset instead of the whole data, then replace \"biblio_df\" with \"field_subset_df\" and run the cell again\n",
    "\n",
    "query_inverse = biblio_df[~biblio_df['100a'].str.contains(\"null\", case=False, regex=True)]\n",
    "\n",
    "query_inverse.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2042fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some info about the subset you got as a result of the query:\n",
    "query_inverse.info(verbose=True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "name_file = 'biblio_author_person_field_100a_notEmpty'\n",
    "\n",
    "query_inverse.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "## or download to csv\n",
    "# query_inverse.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d2b27",
   "metadata": {},
   "source": [
    "# F. Query for a specific record\n",
    "You may want to see the details of a specific record, this can be done in two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb773390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. by using the index position. Example: This item: ToDo has index position 0. \n",
    "# This position can be seen in the left corner of the entire table (cell above in Section5: biblio_df.head(10))\n",
    "# We will query it using the entire version of the data, not the subset\n",
    "\n",
    "# show record vertically using index position\n",
    "query_recordIndex = biblio_df.iloc[0]\n",
    "query_recordIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. By using the record Id using the Marc field 001\n",
    "query_recordId = biblio_df[biblio_df['001'] == '8']\n",
    "query_recordId"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
