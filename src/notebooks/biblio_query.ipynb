{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "Jupyter notebook to query the harvested metadata records from the IISG bibliographic materials (biblio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook makes it possible to get overviews and query the metadata records of the International Institute of Social History (IISG) Bibliographic materials (\"Biblio\"). It uses as source the file \"converted.csv\" obtained via metadata harvesting using the scripts in this repository (https://github.com/lilimelgar/iisg-metadata-overviews).  It contains MARC records from the OAIPMH endpoint. \n",
    "The file contains one record per row, and each marc property (field and subfield) is in a column.\n",
    "\n",
    "Note: the data includes only metadata records at the \"item\" level.\n",
    "\n",
    "Created by Liliana Melgar (April, 2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# A. Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Import the required python libraries \n",
    "*(nothing to change)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# to add timestamp to file names\n",
    "import time\n",
    "# import os.path to add paths to files\n",
    "import os\n",
    "\n",
    "# disable warning for false positive on chained assignment\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Set the path to the csv file \n",
    "*nothing to change if you cloned the repository. If you downloaded the file only (\"biblio_as_csv.gzip\"), then set here the path to where you have downloaded the file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to where the relevant data is located\n",
    "# biblio\n",
    "script_dir = os.getcwd()  # Gets the current working directory\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\", \"..\"))  # Moves up two levels to reach 'repo'\n",
    "data_directory_biblio = os.path.join(project_root, \"data\", \"biblio\")\n",
    "data_converted_biblio = os.path.join(data_directory_biblio, 'converted')\n",
    "data_downloads_biblio = os.path.join(data_directory_biblio, 'downloads') #path to the folder where the reports will be downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Read the csv file as a pandas dataframe\n",
    "*nothing to change here, just be patient, IT TAKES LONG TO LOAD (around started at 19.00h and finished sometime before 20:48h same day)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as dataframe\n",
    "biblio_df_v0 = pd.read_csv(f'{data_converted_biblio}/biblio_as_csv_per_field.gzip', sep=\"\\t\", compression='gzip', low_memory=False)\n",
    "# low_memory=False was set after this warning message: \"/var/folders/3y/xbjxw0b94jxg6x2bcbyjsmmcgvnf7q/T/ipykernel_987/2912965462.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblio_df_v0.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Inspect if import was correct\n",
    "First overview: all fields and data types\n",
    "Execute the cell and view the general information of the data, which includes the Columns (marc properties with subfields), the Non-Null Count (i.e., how many cells have values; for example: if a cell says \"1 non-null\" it means that only one row has a value); and the Data type (object (i.e., a string or a combination of data types), a float or an integer).\n",
    "- Keep in mind that the MARC labels have 3 characters, and that the fourth character can be an indicator or a subfield. For example: 1000 is Marc label 100 with indicator 0. And 100a is Marc label 100 with subfield a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_df_v0.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Columns (documentation)\n",
    "Ideally, each field above would have a definition explaining what it means and what kind of values does it contain (in relation to the conventions for creating IISG metadata). That documentation can exist somewhere else (e.g., on Confluence), but this could be a place to start updating or writing those definitions since here one can see the data that they contain in detail.\n",
    "For now, we can consult the MARC21 documentation which explains what each field label means for Bibliographic data: https://www.loc.gov/marc/bibliographic/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Prepare the data for search\n",
    "Because we know that the data doesn't have proper numerical values to be computed, we rather convert all values to strings in order to facilitate querying. This also includes filling in empty values with a standard string: \"null\"\n",
    "*(nothing to change here)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "df_columns = biblio_df_v0.columns\n",
    "for column in df_columns:\n",
    "    dataType = biblio_df_v0.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].fillna('null')\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].fillna('null')\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].astype(str)\n",
    "    if dataType == object:\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].fillna('null')\n",
    "        biblio_df_v0[column] = biblio_df_v0[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy\n",
    "biblio_df = biblio_df_v0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the csv\n",
    "# biblio_df.to_csv('biblio_all.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again the general information of the data after having filled in the emtpy values and converted the data types\n",
    "biblio_df.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Get a glimpse of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## First rows\n",
    "Here you can see a sample of the records, one per line. You can change the value \"10\" to any other desired size for your sample, preferably not too big. You can also use \"tail\" instead of \"head\" to see the records in the last rows.\n",
    "- Keep in mind to scroll horizontally and vertically to see the entire record.\n",
    "- NaN means that the cell is empty.\n",
    "- Arbitrarily, some cells above, we decided that the omega \"Ω\" would be the separator for multi-value cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Size (shape) of the data\n",
    "Here you can see how many rows (first value) and how many columns (second value) are in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an ok shape would be around 1207000, 238)\n",
    "biblio_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Unique values\n",
    "Here you can see a general description of the data, including how many unique values are per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataframe\n",
    "biblio_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the file\n",
    "# name_file = 'biblio_all_harvested20241117_compr'\n",
    "# biblio_df.to_csv(f'{data_downloads}/{name_file}.csv', index=False, compression='gzip') # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Check the values in one column (marc property)\n",
    "At this point you may be curious to know which values are in one column. It may be interesting to observe those that have very few unique values, because they can contain wrong data or wrong columns.\n",
    "- You can change the field inside the quotation marcs for any other field of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_df['02e'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "If you want to see which record contains those values or wrong columns, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check records with non-empty values in a specific column\n",
    "\n",
    "check_problem1 = biblio_df[~biblio_df['02e'].str.contains(\"null\", case=False, regex=True)]\n",
    "check_problem1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Inspect one record\n",
    "If you are interested to see only one record, you can do so by using the recordId (TCN in 001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST (see one record)\n",
    "# # check if a string value exists in a column (the string is exactly the same)\n",
    "# # test_exact = biblio_df[biblio_df['651a'] == '1362253']\n",
    "# # test_exact = biblio_df[biblio_df['651a'] == 'Srebrenica (Yugoslavia)']\n",
    "\n",
    "check_tcn = '1488082' #'1466360'\n",
    "check_record = biblio_df[biblio_df['001'] == check_tcn]\n",
    "check_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_record.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# # choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "# name_file = 'biblio_651a_Srebrenica'\n",
    "\n",
    "# test_exact.to_excel(f'{data_downloads}/{name_file}.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Create a slice/subset with certain column(s)/field(s)\n",
    "At this point you may have thought that you could perhaps correct some of the records which contain an inconsistent value. For example, in the first version of this data, if you queried above for \"biblio_df['100e'].unique()\" you may have obtained certain values. You may decide that you want to change one or some of them into another value. But for this, you need the TCN (record Id) numbers. The command below facilitates creating a subset with the TCN and the field of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Test first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice for classification fields (690)\n",
    "test1 = biblio_df[['001','690']]\n",
    "test1.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the non-null fields\n",
    "test2 = test1[~test1['690'].str.contains(\"null\", case=False, regex=True)]\n",
    "test2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Divide the main df into two dfs (emtpy/non-empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create subset with record Id and record of interest, here enter the name of the field(s) that you are interested in separated by commas, each field has to be within single quotation marks, e.g., biblio_df[['001','100e', '110e']]\n",
    "# # field_subset_df = biblio_df[['001','090a','901a','245a','245b','260a','852p','852j','866a','902a','leader']] #--> For LA periodicals\n",
    "# # field_subset_df = biblio_df[['001','245a','245b','6510','651a','695g','leader']] #--> For geographic terms exploration\n",
    "# # field_subset_df = biblio_df[['001','090a','901a','245a','245b','260a','852a','852b','852c','852j','852n','852p','866a','902a','leader']] #--> For periodicals Simon report\n",
    "\n",
    "## CREATE SUBSET FOR SUBJECT TERMS ANALYSIS (columns starting with 6)\n",
    "# df_columns = biblio_df.columns\n",
    "# columns_subjects = []\n",
    "# for column in biblio_df.columns:\n",
    "#     if column.startswith(\"6\"):\n",
    "#         columns_subjects.append(column)\n",
    "# field_subset_df = biblio_df[['001', 'leader'] + columns_subjects] #--> For av thesaurus terms\n",
    "\n",
    "# CREATE A SUBSET WITH \"690\" ONLY to evaluate subject terms (classification)\n",
    "field_subset_df_v0 = biblio_df[['001', 'leader','690']] #--> For classification terms\n",
    "# one dataframe for rows with values\n",
    "field_subset_df_v1 = field_subset_df_v0[field_subset_df_v0['690'].str.lower() != 'null'] #fields with values\n",
    "# one dataframe for empty rows\n",
    "field_subset_df_vb = field_subset_df_v0[field_subset_df_v0['690'].str.lower() == 'null'] #fields with empty values (won't be used in this case)\n",
    "\n",
    "# # CREATE A SUBSET WITH \"100\" ONLY to evaluate persons\n",
    "# field_subset_df_v0 = biblio_df[['001', 'leader','100']] #--> For persons\n",
    "# # one dataframe for rows with values\n",
    "# field_subset_df_v1 = field_subset_df_v0[field_subset_df_v0['100'].str.lower() != 'null'] #to exclude empty values\n",
    "# # one dataframe for empty rows\n",
    "# field_subset_df_vb = field_subset_df_v0[field_subset_df_v0['100'].str.lower() == 'null'] #to exclude empty values (not used in this case)\n",
    "\n",
    "# field_subset_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # field_subset_df.info(verbose=True)\n",
    "# field_subset_df.describe()\n",
    "# field_subset_df_vb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2 = field_subset_df_v1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v2.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again the number of unique values in your subset\n",
    "field_subset_df_v2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## Prepare non-empty subset \n",
    "to have one value per row and one subfield per column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Check separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the longest cell (to get the most problematic as example)\n",
    "# # Convert all cells to string and get their lengths\n",
    "# lengths = field_subset_df_v2.astype(str).map(len)\n",
    "\n",
    "# # Find position (row, col) of the max length\n",
    "# max_row, max_col = lengths.stack().idxmax()\n",
    "\n",
    "# # Get the value from the original DataFrame\n",
    "# longest_cell = field_subset_df_v2.loc[max_row, max_col]\n",
    "\n",
    "# print(f\"Longest cell is in row {max_row}, column '{max_col}' with length {len(str(longest_cell))}\")\n",
    "# print(\"Value:\", longest_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the cell where the separator occurs most frequently (to get the most problematic as example)\n",
    "char = '¶'\n",
    "\n",
    "# Count how many times 'e' appears in each cell (as string)\n",
    "char_counts = field_subset_df_v2.astype(str).map(lambda x: x.count(char))\n",
    "\n",
    "# Find the position of the max count\n",
    "max_row, max_col = char_counts.stack().idxmax()\n",
    "\n",
    "# Get the value from the original DataFrame\n",
    "cell_value = field_subset_df_v2.loc[max_row, max_col]\n",
    "count = char_counts.loc[max_row, max_col]\n",
    "\n",
    "print(f\"The character '{char}' appears most in row {max_row}, column '{max_col}' ({count} times)\")\n",
    "print(\"Cell content:\", cell_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get that record Id using row index number\n",
    "# field_subset_df_v2.iloc[182696]\n",
    "field_subset_df_v2.iloc[18941]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the record with the longest value using the TCN (001)\n",
    "# query_value_aprox = field_subset_df_v2[field_subset_df_v2['001'] == '1080191'].copy()\n",
    "# query_value_aprox\n",
    "# test_exact4 = field_subset_df_v2[field_subset_df_v2['001'] == '1466360'] \n",
    "\n",
    "test_exact4 = field_subset_df_v2[field_subset_df_v2['001'] == '130229']\n",
    "test_exact4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Split multi-valued cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into different rows (e.g., explode)\n",
    "\n",
    "# Step 1: Split the column using \"Ω\" as the separator\n",
    "# field_subset_df_v2[\"100\"] = field_subset_df_v2[\"100\"].str.split(\"¶\")\n",
    "field_subset_df_v2[\"690\"] = field_subset_df_v2[\"690\"].str.split(\"¶\")\n",
    "\n",
    "# Step 2: Explode the list into multiple rows\n",
    "# field_subset_df_v2 = field_subset_df_v2.explode(\"100\", ignore_index=True)\n",
    "field_subset_df_v2 = field_subset_df_v2.explode(\"690\", ignore_index=True)\n",
    "\n",
    "field_subset_df_v2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test if the record is split correctly\n",
    "test_exact5 = field_subset_df_v2[field_subset_df_v2['001'] == '130229']\n",
    "test_exact5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape, this should be bigger than the number of records\n",
    "field_subset_df_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v3 = field_subset_df_v2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Split columns per subfield\n",
    "This will make that every subfield goes to its own column, the separator between the subfield label and the value should be checked carefully, I used before \":\" but this caused problems since some values have \":\" in them, thus, now I use also the quotation mark '\":'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE LINES WILL CONVERT FROM THIS FORMAT IN \"DATA\" to a format in which each subfield is in a separate column \n",
    "# with the column name equal to the subfield name, filling in with null the cells where the subfield doesn't exist\n",
    "# data = {\n",
    "#     \"id\": [\"001\", \"002\", \"003\", \"004\"],\n",
    "#     \"values\": ['\"a\":lore;\"b\":ipsum', \n",
    "#                '\"x\":nomine;\"a\":ipsum', \n",
    "#                '\"x\":example;\"c\":dei',\n",
    "#                '\"b\":test;\"y\":test2']\n",
    "# }\n",
    "##########################\n",
    "\n",
    "# # FOR PERSONS (100)\n",
    "# # Parse values into a dictionary-like structure\n",
    "# field_subset_df_v2[\"parsed\"] = field_subset_df_v2[\"100\"].apply(lambda x: {kv.split(\":\")[0]: kv.split(\":\")[1] for kv in x.split(\"⑄\")} if isinstance(x, str) else {})\n",
    "# # Extract all unique keys (column names)\n",
    "# all_keys = sorted(set(k for d in field_subset_df_v2[\"parsed\"] for k in d.keys()))\n",
    "# # all_keys\n",
    "# # Create new columns with values only (remove key names)\n",
    "# for key in all_keys:\n",
    "#     field_subset_df_v2[key] = field_subset_df_v2[\"parsed\"].apply(lambda d: d[key] if key in d else \"null\")\n",
    "# # df\n",
    "# # Keep only relevant columns\n",
    "# field_subset_df_v3 = field_subset_df_v2[[\"001\", \"leader\"] + all_keys]\n",
    "\n",
    "# ####\n",
    "# # FOR SUBJECTS (650)\n",
    "# # Parse values into a dictionary-like structure\n",
    "# field_subset_df_v3[\"parsed\"] = field_subset_df_v3[\"650\"].apply(lambda x: {kv.split('\":')[0]: kv.split('\":')[1] for kv in x.split(\"⑄\")} if isinstance(x, str) else {})\n",
    "# # Extract all unique keys (column names)\n",
    "# all_keys = sorted(set(k for d in field_subset_df_v3[\"parsed\"] for k in d.keys()))\n",
    "# # all_keys\n",
    "# # Create new columns with values only (remove key names)\n",
    "# for key in all_keys:\n",
    "#     field_subset_df_v3[key] = field_subset_df_v3[\"parsed\"].apply(lambda d: d[key] if key in d else \"null\")\n",
    "# # df\n",
    "# # Keep only relevant columns\n",
    "# field_subset_df_v4 = field_subset_df_v3[[\"001\", \"leader\"] + all_keys]\n",
    "\n",
    "\n",
    "####\n",
    "# FOR CLASSIFICATION (690)\n",
    "# Parse values into a dictionary-like structure\n",
    "field_subset_df_v3[\"parsed\"] = field_subset_df_v3[\"690\"].apply(lambda x: {kv.split('\":')[0]: kv.split('\":')[1] for kv in x.split(\"⑄\")} if isinstance(x, str) else {})\n",
    "# Extract all unique keys (column names)\n",
    "all_keys = sorted(set(k for d in field_subset_df_v3[\"parsed\"] for k in d.keys()))\n",
    "# all_keys\n",
    "# Create new columns with values only (remove key names)\n",
    "for key in all_keys:\n",
    "    field_subset_df_v3[key] = field_subset_df_v3[\"parsed\"].apply(lambda d: d[key] if key in d else \"null\")\n",
    "# df\n",
    "# Keep only relevant columns\n",
    "field_subset_df_v4 = field_subset_df_v3[[\"001\", \"leader\"] + all_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v4.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v5 = field_subset_df_v4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RENAME COLUMNS\n",
    "df_columns = field_subset_df_v5.columns\n",
    "# df_columns\n",
    "field_subset_df_v5.columns = field_subset_df_v5.columns.str.replace('\"', '', regex=False).str.strip()\n",
    "# field_subset_df_v5.rename(columns={'650': '650', 'leader': 'leader', '0': 'indicator_0', '1': 'indicator_1', '2': 'indicator_2', '4': 'indicator_4', '6': 'indicator_6', '8': 'indicator_8', 'a': 'subfield_a', 'd': 'subfield_d', 'g': 'subfield_g', 'l': 'subfield_l', 'v': 'subfield_v', 'x': 'subfield_x', 'y': 'subfield_y', 'z': 'subfield_z'}, inplace=True) --> FOR SUBJECTS\n",
    "field_subset_df_v5.rename(columns={'690': '690', 'leader': 'leader', '\"a': 'subfield_a', '\"b': 'subfield_b', '\"c': 'subfield_c'}, inplace=True)\n",
    "field_subset_df_v5.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "## Correct problems (if there are)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the lines below check what the problem is when an \"empty\" column appears, i.e., a column that is not expected, which means something went wrong with the parsing.\n",
    "# field_subset_df_problem1 = field_subset_df_v5.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "<!-- If the split shows some strange columns, you may need to inspect if it went well, for that it may be useful to check the unique values in the suspicious column, but since column names contain quotation marks, it is best to rename them first: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RENAME COLUMNS\n",
    "# df_columns = field_subset_df_problem1.columns\n",
    "# # df_columns\n",
    "# field_subset_df_problem1.columns = field_subset_df_v5.columns.str.replace('\"', '', regex=False).str.strip()\n",
    "# field_subset_df_problem1.rename(columns={'650': '650', 'leader': 'leader', '': 'empty', '0': 'indicator_0', '1': 'indicator_1', '2': 'indicator_2', '4': 'indicator_4', '6': 'indicator_6', '8': 'indicator_8', 'a': 'subfield_a', 'd': 'subfield_d', 'g': 'subfield_g', 'l': 'subfield_l', 'v': 'subfield_v', 'x': 'subfield_x', 'y': 'subfield_y', 'z': 'subfield_z'}, inplace=True)\n",
    "# field_subset_df_problem1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "<!-- Now you can inspect the column's unique values -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# field_subset_df_problem1['empty'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### Report problematic records (now drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "<!-- To see which records are problematic based on the previous inspection use: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE\n",
    "# to_correct_df = field_subset_df_v5\n",
    "# to_correct_df.describe()\n",
    "\n",
    "# to_correct_df[to_correct_df['\" \"'].notnull()]\n",
    "# # to_correct_df[to_correct_df['\"1\"'].notnull()]\n",
    "# # to_correct_df['\"1\"'].unique()\n",
    "# test_record2 = to_correct_df[to_correct_df['\"1\"'].str.contains(\"http\", case=False, regex=True)]\n",
    "# test_record2\n",
    "# to_correct_df2 = to_correct_df.drop(199050).reset_index(drop=True).copy()\n",
    "# to_correct_df2.info()\n",
    "# # delete\n",
    "# to_correct_df3 = to_correct_df2.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: check what it's in the column with no value \" \" --> Renamed to \"empty\"\n",
    "# # check = field_subset_df_v3[['001','empty']]\n",
    "# # check_problem.describe()\n",
    "\n",
    "# check_problem = to_correct_df[~to_correct_df['empty'].str.contains(\"null\", case=False, regex=True)]\n",
    "# check_problem.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inspect the problematic record in the originally imported csv\n",
    "# check_problem_record = biblio_df_v0[biblio_df_v0['001'] == '1529613'] #'1466360'\n",
    "# check_problem_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get row number of problematic record in the sliced dataframe\n",
    "# check_tcn = '1529613'\n",
    "# check_record = field_subset_df_v5[field_subset_df_v5['001'] == check_tcn]\n",
    "# check_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPORT THESE RECORDS TO MIEKE (APRIL 17, 2025)\n",
    "1529613"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMPORARILY DROP THE OUTLIER rows which contain the same TCN\n",
    "# field_subset_df_v6 = field_subset_df_v5.drop(field_subset_df_v5[field_subset_df_v5['001'] == '1529613'].index)\n",
    "# field_subset_df_v6.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_problem_again = field_subset_df_v6[~field_subset_df_v6['empty'].str.contains(\"null\", case=False, regex=True)]\n",
    "# check_problem_again.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if it's all corrected, remove the columns that are empty\n",
    "# field_subset_df_v6.drop(['empty'], axis='columns', inplace=True)\n",
    "# field_subset_df_v7 = field_subset_df_v6.reset_index(drop=True).copy()\n",
    "# field_subset_df_v7.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "## Concatenate the dataframes again (empty/non-empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL HERE: if some problems were corrected, make a copy of version 7, if not, make a copy of version 5 (before correcting: step 4.4)\n",
    "field_subset_df_v8 = field_subset_df_v5.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special for when there were two separated dfs one with non-empty values one with empty values, this is useful to generate\n",
    "# report of empty fields, especially for the pie chart with records that don't have any value in 650a\n",
    "\n",
    "# create column 650 with 'null/notnull' values to be able to filter\n",
    "field_subset_df_v8[\"690\"] = 'notnull'\n",
    "frames = [field_subset_df_v8, field_subset_df_vb]\n",
    "field_subset_df_v9 = pd.concat(frames, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it went well\n",
    "field_subset_df_v9['690'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v9.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "df_columns = field_subset_df_v9.columns\n",
    "for column in df_columns:\n",
    "    dataType = field_subset_df_v9.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        field_subset_df_v9[column] = field_subset_df_v9[column].fillna('null')\n",
    "        field_subset_df_v9[column] = field_subset_df_v9[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        biblio_field_subset_df_v9df_v0[column] = field_subset_df_v9[column].fillna('null')\n",
    "        field_subset_df_v9[column] = field_subset_df_v9[column].astype(str)\n",
    "    if dataType == object:\n",
    "        field_subset_df_v9[column] = field_subset_df_v9[column].fillna('null')\n",
    "        field_subset_df_v9[column] = field_subset_df_v9[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df_v10 = field_subset_df_v9.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "## Split the leader code\n",
    "to be able to filter per media type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the leader code to be able to generate report per item type\n",
    "# split using character position, remember the leader is at position 6 to 8\n",
    "field_subset_df_v10[\"leader_code\"] = field_subset_df_v10[\"leader\"].map(lambda x: x[5:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it went well\n",
    "field_subset_df_v10['leader_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if it went well\n",
    "field_subset_df_v10.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "## Download slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df = field_subset_df_v10.reset_index(drop=True)\n",
    "field_subset_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "# name_file = 'biblio_author_person_field_100a' #--> authors test\n",
    "# name_file = 'biblio_geo_651a' #--> geoterms\n",
    "# name_file = 'biblio_serials_report_simon' #--> serials report for Simon 2024-11-14\n",
    "# name_file = 'subjects_600_subfields' # for thesaurus report (202504? and 20250414)\n",
    "# name_file = 'persons_100_subfields' # for thesaurus report (20250414)\n",
    "name_file = 'subjects_690_subfields' # for classification report (20251125)\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# field_subset_df.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "## or download to csv\n",
    "field_subset_df.to_csv(f'{data_downloads_biblio}/{name_file}_{timestr}.csv', index=False) # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# # choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "# # name_file = 'biblio_author_person_field_100a' #--> authors test\n",
    "# # name_file = 'biblio_geo_651a' #--> geoterms\n",
    "# # name_file = 'authorities_geo_151a_parenthesis'\n",
    "# # name_file = 'subject_terms_per_150'\n",
    "# name_file = 'person_names_per_100'\n",
    "\n",
    "# # field_subset_df.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "# ## or download to csv\n",
    "# field_subset_df_v3.to_csv(f'{data_downloads_authority}/{name_file}.csv', index=False) # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "# Create a subset of records with a certain value in a given column (toDo)\n",
    "You may also want to create a list of the records with a certain value in a given column, for example, for field 100e you got these unique values: ['creator.', 'null', 'creator']. You may want to get only the list of records that have \"creator.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when the file above is too big, it's useful sometimes to download it and upload it here again\n",
    "path = '/Users/lilianam/workspace/iisg-metadata-overviews/biblio/data/downloads'\n",
    "# field_subset_df = pd.read_csv(f'{path}/biblio_titles.csv.gz', sep=\",\", compression='gzip', low_memory=False)\n",
    "\n",
    "field_subset_df = pd.read_csv(f'{path}/biblio_serials_report_simon.csv', sep=\",\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_subset_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if a string value exists in a column (the string is exactly the same)\n",
    "# query_value_exact = field_subset_df[field_subset_df['100a'] == 'Hajnal, Henri.']\n",
    "# query_value_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if a string value exists in a column (the string is approximately the same)\n",
    "# you may want to find the records that have either \"creator.\" (with dot) or \"creator\" without dot, but not the null values\n",
    "# here it's possible to use regular expressions\n",
    "\n",
    "# query_value_aprox = field_subset_df[field_subset_df['852j'].str.contains(\"ZDF|ZF|ZDK|ZO|XZK|ZDO|ZK\", case=True, regex=True)] # for LA periodicals\n",
    "\n",
    "# query_value_aprox2 = (field_subset_df[field_subset_df['leader'].str.contains('cas|nas') & field_subset_df['852c'].str.contains('NIBG')]) --> Simon report, not good\n",
    "\n",
    "query_value_aprox3 = field_subset_df[field_subset_df['leader'].str.contains(\"cas|nas\", case=True, regex=True)] # for simon report v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_value_aprox3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some idea of how many rows are in this set\n",
    "query_value_aprox3.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "serials_subset = query_value_aprox3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "df_columns = serials_subset.columns\n",
    "for column in df_columns:\n",
    "    dataType = serials_subset.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        serials_subset[column] = serials_subset[column].fillna('null')\n",
    "        serials_subset[column] = serials_subset[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        serials_subset[column] = serials_subset[column].fillna('null')\n",
    "        serials_subset[column] = serials_subset[column].astype(str)\n",
    "    if dataType == object:\n",
    "        serials_subset[column] = serials_subset[column].fillna('null')\n",
    "        serials_subset[column] = serials_subset[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check again the number of unique values in your subset\n",
    "serials_subset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "serials_subset.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "serials_subset.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "# name_file = 'biblio_author_person_field_100a_henri'\n",
    "# name_file = 'biblio_to_map_la_periodicals_852j'\n",
    "\n",
    "name_file = 'biblio_serials_simon_report_v4'\n",
    "\n",
    "# serials_subset.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "## or download to csv\n",
    "# query_value_aprox.to_csv()\n",
    "serials_subset.to_csv(f'{data_downloads}/{name_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "# Create subsets using inverse query (toDo)\n",
    "You may need to create a report with all the records that do not contain a certain value. For example, because we used \"null\" to fill in all empty values, one could create a list with all the records that have a value in a certain column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a slice with the records that have non-null values in the column of interest\n",
    "# Note: if you want to query the subset instead of the whole data, then replace \"biblio_df\" with \"field_subset_df\" and run the cell again\n",
    "\n",
    "query_inverse = biblio_df[~biblio_df['100a'].str.contains(\"null\", case=False, regex=True)]\n",
    "\n",
    "query_inverse.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some info about the subset you got as a result of the query:\n",
    "query_inverse.info(verbose=True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may want to dowload the table above to an excel file for further inspection:\n",
    "\n",
    "# choose any name for your file, the file will go to the ../data/downloads folder.\n",
    "name_file = 'biblio_author_person_field_100a_notEmpty'\n",
    "\n",
    "query_inverse.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "## or download to csv\n",
    "# query_inverse.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "# Query for specific records (toDo)\n",
    "You may want to see the details of specific records, this can be done in two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. by using the index position. Example: This item: ToDo has index position 0. \n",
    "# This position can be seen in the left corner of the entire table (cell above in Section5: biblio_df.head(10))\n",
    "# We will query it using the entire version of the data, not the subset\n",
    "\n",
    "# show record vertically using index position\n",
    "query_recordIndex = biblio_df.iloc[0]\n",
    "query_recordIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. By using the record Id using the Marc field 001\n",
    "query_recordId = biblio_df[biblio_df['001'] == '8']\n",
    "query_recordId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "# TEST SQL CONNECTION (toDo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite://', echo=False)\n",
    "\n",
    "field_subset_df.to_sql(name='testdb', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"SELECT * FROM testdb\")).fetchall()\n",
    "    [(0, '001'), (1, '245a'), (2, '245b')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"PRAGMA table_info(testdb)\")).fetchall()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST SQL ENDS HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
