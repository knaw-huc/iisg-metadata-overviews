{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "Jupyter notebook to get an overview of the different classification terms used at the IISH from four different sources\n",
    "- dateCreated: 2025-11-24\n",
    "- creator: Liliana Melgar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# import os.path to add paths to files\n",
    "import os\n",
    "\n",
    "# FOR VENN DIAGRAMS\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "from matplotlib_venn import venn3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Set paths to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to where the relevant data is located\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\", \"..\"))  # Moves up two levels to reach 'repo'\n",
    "\n",
    "# biblio\n",
    "data_directory_biblio = os.path.join(project_root, \"data\", \"biblio\")\n",
    "data_downloads_biblio = os.path.join(data_directory_biblio, 'downloads') #path to the folder where the reports will be downloaded\n",
    "\n",
    "# authority\n",
    "script_dir = os.getcwd()  # Gets the current working directory\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\", \"..\"))  # Moves up two levels to reach 'repo'\n",
    "data_directory_authority = os.path.join(project_root, \"data\", \"authority\")\n",
    "data_downloads_authority = os.path.join(data_directory_authority, 'downloads') #path to the folder where the reports will be downloaded\n",
    "\n",
    "# subjects (thesauri)\n",
    "script_dir = os.getcwd()  # Gets the current working directory\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\", \"..\"))  # Moves up two levels to reach 'repo'\n",
    "data_directory_subjects = os.path.join(project_root, \"data\", \"subjects\")\n",
    "data_downloads_subjects = os.path.join(data_directory_subjects, 'downloads') #path to the folder where the reports will be downloaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# BIBLIO classification terms overview\n",
    "- Biblio is ...\n",
    "- These terms are extracted from the IISH metadata using the public version of the OAI-PMH endpoint. For more information about what BIBLIO contains, see: https://confluence.socialhistoryservices.org/x/S4FeBw.\n",
    "- The harvesting was done using the code from the \"Metadata overviews\" repository: https://github.com/lilimelgar/iisg-metadata-overviews\n",
    "- **The harvesting date was September 26, 2025**.\n",
    "- All the records from the catalog are included, but not all the columns, only the 690 field was included. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Read csv file\n",
    "This csv file was created using another jupyter notebook (https://github.com/lilimelgar/iisg-metadata-overviews/blob/main/biblio/src/biblio_query.ipynb). It creates a slice of the entire MARC metadata from Evergreen by selecting only the MARC field 650, because 650 corresponds to the subject terms in MARC (https://www.loc.gov/marc/bibliographic/bd650.html). In that notebook, the repeated fields and subfields are split into separate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as dataframe\n",
    "biblio_690_df_v0 = pd.read_csv(f'{data_downloads_biblio}/subjects_690_subfields_20251124-180647.csv', sep=\",\", low_memory=False)\n",
    "\n",
    "# low_memory=False was set after this warning message: \"/var/folders/3y/xbjxw0b94jxg6x2bcbyjsmmcgvnf7q/T/ipykernel_987/2912965462.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\"\n",
    "\n",
    "# history of use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Inspect if import was correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the subfields and the number of filled-in values for field 650\n",
    "# 001 is the recordId (which is called TCN in Evergreen)\n",
    "biblio_690_df_v0.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Fill in emtpy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "df_columns = biblio_690_df_v0.columns\n",
    "for column in df_columns:\n",
    "    dataType = biblio_690_df_v0.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        biblio_690_df_v0[column] = biblio_690_df_v0[column].fillna('null')\n",
    "        biblio_690_df_v0[column] = biblio_690_df_v0[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        biblio_690_df_v0[column] = biblio_690_df_v0[column].fillna('null')\n",
    "        biblio_690_df_v0[column] = biblio_690_df_v0[column].astype(str)\n",
    "    if dataType == object:\n",
    "        biblio_690_df_v0[column] = biblio_690_df_v0[column].fillna('null')\n",
    "        biblio_690_df_v0[column] = biblio_690_df_v0[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "biblio_690_df = biblio_690_df_v0.copy()\n",
    "biblio_690_df.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Overview of Biblio's 690 field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape shows the number of rows and columns\n",
    "shape = biblio_690_df.shape\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shows the number of unique records using the identifier (TCN) column from Evergreen records\n",
    "number_of_records = biblio_690_df['001'].nunique()\n",
    "print(number_of_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation\n",
    "print(f\"Shape = {shape}: This value shows that the data contains {shape[0]} rows and {shape[1]} columns. The number of rows is not the same as the number of records ({number_of_records}) in the catalog since the rows were split in a way that, if the record had a 690 field with multiple values, each value was put in a separate row.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# head shows the first 5 records (if you want to see more records, change 10 for another value)\n",
    "biblio_690_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_690_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### SPECIFIC QUERIES TO BIBLIO 690"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Retrieve record by TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in any record Id (TCN number) you want to explore\n",
    "example_record_tcn = '130229'\n",
    "example_record = biblio_690_df[biblio_690_df['001'] == example_record_tcn] \n",
    "example_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### Find terms in 690"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Above one can see record with Id {example_record_tcn} which is here in the online catalog: https://search.iisg.amsterdam/Record/{example_record_tcn}. Below one can see the records that contain a specific string, e.g., 'ISR', here you can use regular expressions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_records = biblio_690_df[biblio_690_df['subfield_a'].str.contains(\"ISR\", case=False, regex=True)]\n",
    "search_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_records.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all variants of the term in the 650a subfield\n",
    "search_unique_values = search_records['subfield_a'].value_counts(sort=True)\n",
    "\n",
    "search_unique_values\n",
    "\n",
    "# values_list = search_unique_values.tolist()\n",
    "\n",
    "# # sort alphabetically\n",
    "# values_list_sorted = sorted(values_list, key=str.lower)\n",
    "\n",
    "# for line in values_list_sorted:\n",
    "#     print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find records with a given classification\n",
    "search_records_classification = biblio_690_df[biblio_690_df['subfield_a'].str.fullmatch(\"IC ISR 104\", case=False, na=False)]\n",
    "\n",
    "\n",
    "# biblio_650_df[biblio_650_df['001'] == example_record_tcn2] \n",
    "\n",
    "\n",
    "search_records_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# records_isr = biblio_690_df[biblio_690_df['subfield_a'].str.contains(\"ISR\", case=False, regex=True)]\n",
    "# records_isr\n",
    "\n",
    "# group records to get TCNs for each classification\n",
    "\n",
    "biblio_690_df['001'] = biblio_690_df['001'].astype(str)\n",
    "\n",
    "records_isr_v0 = biblio_690_df[biblio_690_df['subfield_a'].str.contains(\"IC ISR\", case=False, regex=True)]\n",
    "\n",
    "records_isr_tcn = (\n",
    "    records_isr_v0\n",
    "    .groupby('subfield_a', as_index=False)\n",
    "    .agg(\n",
    "        count_of_records=('001', 'nunique'),\n",
    "        ids=('001', list),\n",
    "    )\n",
    ")\n",
    "\n",
    "records_isr_tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "name_file = 'tcns_classification_ic-isr_20251125'\n",
    "\n",
    "\n",
    "# ## or download to csv\n",
    "records_isr_tcn.to_csv(f'{data_downloads_biblio}/{name_file}.csv', index=False) # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Records with subjects\n",
    "This section focuses on the presence or absense of subject terms (in Marc field 650) in ALL the Catalog records (Biblio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Number of records with/without subject terms in 650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subset of biblio with relevant columns\n",
    "biblio_records_subjects_v0 = biblio_650_df[['001','indicator_0', '650', 'leader_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_records_subjects_v0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of 'null' and 'notNull'\n",
    "value_counts = biblio_records_subjects_v0.groupby('650')['001'].nunique()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total unique '001' count (to check that it's all correct the total should be the entire number of records in the catalog)\n",
    "total_unique_ids = biblio_records_subjects_v0['001'].nunique()\n",
    "total_unique_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Plot of records with/without 650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot (Label mapping)\n",
    "labels_mapping = {\n",
    "    'notnull': f'records with field 650 ({value_counts.get(\"notnull\", 0)})',\n",
    "    'null': f'records without field 650 ({value_counts.get(\"null\", 0)})'\n",
    "}\n",
    "custom_labels = [labels_mapping[label] for label in value_counts.index]\n",
    "\n",
    "# Colors\n",
    "colors = ['#90ee90','#cccccc']\n",
    "\n",
    "# Step 4: Plot pie chart\n",
    "fig1, ax = plt.subplots(figsize=(6,6))\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    value_counts, labels=custom_labels, autopct='%1.1f%%', colors=colors, startangle=90,\n",
    "    wedgeprops={'edgecolor': 'white', 'linewidth': 2}, pctdistance=0.85\n",
    ")\n",
    "\n",
    "# Donut hole\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig1.gca().add_artist(centre_circle)\n",
    "\n",
    "# Center text\n",
    "ax.text(0, 0, f'Total number of records in Biblio\\n{total_unique_ids}', \n",
    "        ha='center', va='center', fontsize=7, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To save the image\n",
    "# name_file = 'plot1_recordsWithSubject650-'\n",
    "# fig1.savefig(f'{data_downloads_subjects}/{name_file}.png', format='png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Number of records with/without subject terms in 650 per media type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "The following table and plots show the number of records that have/don't have subject terms (field 650) distributed per media type. The media type is encoded in the Marc leader. An explanation of the codes can be found here: https://confluence.socialhistoryservices.org/x/OoJAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by null and notnull and leader code\n",
    "# value_counts_with_leader = biblio_records_subjects_v0.groupby(['650', 'leader_code'])['001'].count()\n",
    "# table_counts_df = value_counts_with_leader.reset_index(name='count')\n",
    "\n",
    "# Step 1: Group and count\n",
    "value_counts_media_type = (\n",
    "    biblio_records_subjects_v0\n",
    "    .groupby(['650', 'leader_code'])['001']\n",
    "    .count()\n",
    "    .reset_index(name='count')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Pivot the table\n",
    "pivot_df = value_counts_media_type.pivot_table(\n",
    "    index='650',\n",
    "    columns='leader_code',\n",
    "    values='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Optional: Order rows as 'notnull' then 'null'\n",
    "pivot_df = pivot_df.reindex(['notnull', 'null'])\n",
    "\n",
    "# Step 4: Display nicely in Jupyter\n",
    "from IPython.display import display\n",
    "\n",
    "styled = (\n",
    "    pivot_df.style\n",
    "        .set_caption(\"Record Counts by Leader Code and 650 Field Presence\")\n",
    "        .format('{:,}')\n",
    "        .background_gradient(cmap='Greens')\n",
    "        .set_properties(**{'text-align': 'center'})\n",
    ")\n",
    "\n",
    "display(styled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Plot of records with/without 650 per media type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Step 1: Create long-form dataframe from your pivot table\n",
    "long_df = pivot_df.reset_index().melt(id_vars='650', var_name='leader_code', value_name='count')\n",
    "\n",
    "# Step 2: Get total NOTNULL count per leader_code\n",
    "notnull_order = (\n",
    "    long_df[long_df['650'] == 'notnull']\n",
    "    .sort_values('count', ascending=False)\n",
    "    ['leader_code']\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "# Step 3: Convert 'leader_code' column to ordered category\n",
    "long_df['leader_code'] = pd.Categorical(long_df['leader_code'], categories=notnull_order, ordered=True)\n",
    "\n",
    "# Step 4: Plot as before\n",
    "fig = px.bar(\n",
    "    long_df,\n",
    "    x='leader_code',\n",
    "    y='count',\n",
    "    color='650',\n",
    "    barmode='stack',\n",
    "    text='count',\n",
    "    title='Stacked Bar Chart Sorted by Notnull Count',\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(type='category'),  # ensures equal bar width\n",
    "    yaxis_title='Number of Records',\n",
    "    xaxis_title='Leader Code',\n",
    "    bargap=0.15\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.treemap(\n",
    "    value_counts_media_type,\n",
    "    path=['650', 'leader_code'],  # hierarchy levels\n",
    "    values='count',\n",
    "    title='Treemap: Record Distribution by Subject Presence and Code'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.bar(\n",
    "    value_counts_media_type,\n",
    "    x='leader_code',\n",
    "    y='count',\n",
    "    color='650',\n",
    "    barmode='group',\n",
    "    title='Grouped Bar Chart: Record Counts by Code and 650'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "## Unique subject terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Terms per frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df groupping per term showing the counts\n",
    "# Group by 'term' and count unique '001' values for each term\n",
    "biblio_subjectUniqueTerms_v0 = biblio_650_df.groupby('subfield_a', as_index=False).agg(\n",
    "    count_of_records=('001', 'nunique'),\n",
    "    ids=('001', lambda x: ','.join(map(str, x)))\n",
    ")\n",
    "\n",
    "# Sort by 'count_of_ids' in descending order (most frequent to least frequent)\n",
    "biblio_subjectUniqueTerms_v1 = biblio_subjectUniqueTerms_v0.sort_values(by='count_of_records', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove column with the record Ids\n",
    "biblio_subjectUniqueTerms_v2 = biblio_subjectUniqueTerms_v1[['subfield_a','count_of_records']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the row that contains the null values\n",
    "biblio_subjectUniqueTerms_v3 = biblio_subjectUniqueTerms_v2[biblio_subjectUniqueTerms_v2['subfield_a'] != 'null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v4 = biblio_subjectUniqueTerms_v3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine how many subjects should be shown\n",
    "top_r = 20\n",
    "# create small df for displaying and plotting\n",
    "biblio_subjectUniqueTerms_top = biblio_subjectUniqueTerms_v4.head(top_r).reset_index(drop=True).copy()\n",
    "\n",
    "# plotting in a barh chart the top n terms\n",
    "fig2, ax = plt.subplots(figsize=(20, 10))  # Create the figure object\n",
    "ax = biblio_subjectUniqueTerms_top.groupby(['subfield_a'])['count_of_records'].sum().sort_values(ascending=True).tail(top_r).plot(kind='barh', figsize=(20, 10))\n",
    "ax.set_title(\"Top terms in Biblio's 650$a field\")\n",
    "ax.set_xlabel(\"Number biblio records\")\n",
    "ax.set_ylabel(\"Term $a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the figure\n",
    "# name_file = 'plot2_uniqueSubjectTerms650a--'\n",
    "# # Save the figure as PNG\n",
    "# fig2.savefig(f'{data_downloads_subjects}/{name_file}.png', format='png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # export all list of unique subject terms with the number of occurrences in Biblio\n",
    "# biblio_subjectUniqueTerms_v1.rename(columns={'subfield_a': '650a', 'count_of_records': 'count_records_biblio', 'ids': 'biblio_record_ids'}, inplace=True)\n",
    "\n",
    "# timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "# name_file = 'unique_650a_with_counts_and_recordIds'\n",
    "\n",
    "# biblio_subjectUniqueTerms_v1.to_csv(f'{data_downloads_subjects}/{name_file}_{timestr}.csv', index=False) # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms = biblio_subjectUniqueTerms_v4.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_unique_terms_biblio = biblio_subjectUniqueTerms['subfield_a'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_unique_terms_biblio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## Normalize lists for Venn diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Normalize strings for Venn diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize strings\n",
    "biblio_subjectUniqueTerms['concept_string_normalized'] = (biblio_subjectUniqueTerms['subfield_a'].str.replace('-', ' ', regex=False).copy()   # Remove dashes\n",
    "                                                                                                 .str.title().copy()                         # Convert to title case\n",
    "                                                                                                 .str.replace(' ', '', regex=False).copy()  # Remove spaces\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test_venn0 = biblio_subjectUniqueTerms[biblio_subjectUniqueTerms['subfield_a'].str.contains(\"market\", case=False, regex=True)] #\n",
    "query_test_venn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_tcn = '717530'\n",
    "# check_record = biblio_subjectUniqueTerms[biblio_subjectUniqueTerms['001'] == check_tcn]\n",
    "# check_record\n",
    "\n",
    "biblio_subjectUniqueTerms[biblio_subjectUniqueTerms['subfield_a'].str.contains(\"Typists\", case=False, regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in any record Id (TCN number) you want to explore\n",
    "example_record_tcn2 = '1005655' #'1528765'\n",
    "example_record2 = biblio_650_df[biblio_650_df['001'] == example_record_tcn2] \n",
    "example_record2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjects_list = biblio_subjectUniqueTerms['concept_string_normalized'].tolist()\n",
    "len(biblio_subjects_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Normalize the indicator to get AuthorityId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v30 = biblio_650_df[['001','indicator_0','subfield_a']]\n",
    "biblio_subjectUniqueTerms_v30.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the row that contains the null values\n",
    "biblio_subjectUniqueTerms_v50 = biblio_subjectUniqueTerms_v30[biblio_subjectUniqueTerms_v30['indicator_0'] != 'null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only for the local id (to exclude OCLC or other Ids)\n",
    "biblio_subjectUniqueTerms_v55 = biblio_subjectUniqueTerms_v50[biblio_subjectUniqueTerms_v50['indicator_0'].str.contains(\"\\(NL-AMISG\\)\", case=False, regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v56 = biblio_subjectUniqueTerms_v55.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v56['indicator_0_normalized'] = (biblio_subjectUniqueTerms_v56['indicator_0'].str.replace('(NL-AMISG)', '', regex=False).copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v56.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df groupping per term showing the counts\n",
    "# Group by 'term' and count unique '001' values for each term\n",
    "\n",
    "## aggregates all values from 'subfield_a'\n",
    "# biblio_subjectUniqueTerms_v52 = biblio_subjectUniqueTerms_v51.groupby('indicator_0_normalized', as_index=False).agg(\n",
    "#     count_of_records=('001', 'nunique'),\n",
    "#     strings=('subfield_a', lambda x: ','.join(map(str, x)))\n",
    "# )\n",
    "\n",
    "#keeps unique values from subfield_a\n",
    "biblio_subjectUniqueTerms_v57 = biblio_subjectUniqueTerms_v56.groupby('indicator_0_normalized', as_index=False).agg(\n",
    "    count_of_records=('001', 'nunique'),\n",
    "    unique_strings=('subfield_a', lambda x: len(set(x.dropna()))),\n",
    "    strings=('subfield_a', lambda x: ','.join(sorted(set(x.dropna()))))\n",
    ")\n",
    "\n",
    "# Sort by 'count_of_ids' in descending order (most frequent to least frequent)\n",
    "biblio_subjectUniqueTerms_v58 = biblio_subjectUniqueTerms_v57.sort_values(by='count_of_records', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v58.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v59 = biblio_subjectUniqueTerms_v58.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v59.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v59.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjects_Ids_list = biblio_subjectUniqueTerms_v59['indicator_0_normalized'].tolist()\n",
    "len(biblio_subjects_Ids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjects_Ids_wrong = biblio_subjectUniqueTerms_v59[biblio_subjectUniqueTerms_v59['unique_strings'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjects_Ids_wrong.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjects_Ids_wrong.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = biblio_subjects_Ids_wrong['count_of_records'].sum()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "## Problems with unique subject terms in Biblio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "This section looks a bit more in detail to the overview given in Section 3.2 (Overview of Biblio's 650 field)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "### Orphan subjects \n",
    "These are subject terms without an identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique value counts per column \"subfield_a\" \n",
    "subfield_a_unique = biblio_650_df['subfield_a'].nunique()\n",
    "indicator_0_unique = biblio_650_df['indicator_0'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanation\n",
    "print(f\"We see that the value counts per subfield_a ({subfield_a_unique}) and indicator_0 ({indicator_0_unique}) differ. Because indicator_0 is an identifier for the term in subfield_a, one would expect to have a one to one relation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "The following example record illustrates the issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test record\n",
    "test_record = biblio_650_df[biblio_650_df['001'].str.contains(\"1021749\", case=False, regex=True)] \n",
    "test_record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "In the previous record, every line corresponds to an instance of field 650$a. As we see, in row 4187 there is a subject term that has an identifier from OCLC, and on line 4192 an idenfitier from the IISH (which comes from the Authority database. But we also see several subject terms that have no \"indicator_0\". These terms are not connected to the Authority database or to any other thesaurus or subject list via an identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many subjects have this problem?\n",
    "\n",
    "subjects_without_ids = biblio_650_df[\n",
    "    (biblio_650_df['indicator_0'].str.lower() == \"null\") &\n",
    "    (biblio_650_df['subfield_a'].str.lower() != \"null\")\n",
    "]\n",
    "\n",
    "shape = subjects_without_ids.shape\n",
    "unique_records = subjects_without_ids['001'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This shows that {shape[0]} subjects don't have an identifier, which occurs in {unique_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But some may have an identifier which is external, thus, count how many are those\n",
    "\n",
    "subjects_with_ids = biblio_650_df[\n",
    "    (biblio_650_df['indicator_0'].str.lower() != \"null\") &\n",
    "    (biblio_650_df['subfield_a'].str.lower() != \"null\")\n",
    "]\n",
    "\n",
    "subjects_without_local_ids = subjects_with_ids[~subjects_with_ids['indicator_0'].str.contains(\"AMISG\", case=False, regex=True)] \n",
    "subjects_without_local_ids.head(10)\n",
    "# subjects_without_local_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_without_local_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_records_no_local = subjects_without_local_ids['001'].nunique()\n",
    "unique_records_no_local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "### Unstructured subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the longest cell (to get the most problematic as example)\n",
    "# # Convert all cells to string and get their lengths\n",
    "# lengths = biblio_650_df['subfield_a'].astype(str).map(len)\n",
    "\n",
    "# # Find position (row, col) of the max length\n",
    "# max_row, max_col = lengths.stack().idxmax()\n",
    "\n",
    "# # Get the value from the original DataFrame\n",
    "# longest_cell = biblio_650_df.loc[max_row, max_col]\n",
    "\n",
    "# print(f\"Longest cell is in row {max_row}, column '{max_col}' with length {len(str(longest_cell))}\")\n",
    "# print(\"Value:\", longest_cell)\n",
    "\n",
    "test_terms = biblio_650_df[biblio_650_df['subfield_a'].str.contains(\";+\", case=False, regex=True)]\n",
    "test_terms.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate report of recordIds where this problem occurs\n",
    "# test_term1['001'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "### Subjects with same string and multiple Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I should exclude the Ids that are external, e.g., OCLC...\n",
    "\n",
    "subjects_with_local_ids = subjects_with_ids[subjects_with_ids['indicator_0'].str.contains(\"AMISG\", case=False, regex=True)] \n",
    "subjects_with_local_ids.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects_with_local_ids['indicator_0'].value_counts() --> this shows some messy local identifiers mixed with external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_with_local_ids['001'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a df groupping per term showing the counts\n",
    "# # Group by 'term' and count unique '001' values for each term\n",
    "# biblio_subjectUniqueTerms_v0 = biblio_650_df.groupby('subfield_a', as_index=False).agg(\n",
    "#     count_of_records=('001', 'nunique'),\n",
    "#     ids=('001', lambda x: ','.join(map(str, x)))\n",
    "# )\n",
    "\n",
    "# # Sort by 'count_of_ids' in descending order (most frequent to least frequent)\n",
    "# biblio_subjectUniqueTerms_v1 = biblio_subjectUniqueTerms_v0.sort_values(by='count_of_records', ascending=False)\n",
    "\n",
    "biblio_subjectUniqueTerms_v20 = biblio_650_df.groupby('subfield_a', as_index=False).agg(\n",
    "    count_of_records=('001', 'nunique'),\n",
    "    ids=('001', lambda x: ','.join(map(str, x))),\n",
    "    unique_ids =('indicator_0', lambda x: len(set(x.dropna()))),\n",
    "    indicator_0=('indicator_0', lambda x: ','.join(sorted(set(x.dropna()))))\n",
    ")\n",
    "\n",
    "\n",
    "# Sort by 'count_of_ids' in descending order (most frequent to least frequent)\n",
    "biblio_subjectUniqueTerms_v21 = biblio_subjectUniqueTerms_v20.sort_values(by='count_of_records', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v21.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove column with the record Ids\n",
    "biblio_subjectUniqueTerms_v22 = biblio_subjectUniqueTerms_v21[['subfield_a', 'indicator_0', 'unique_ids', 'ids', 'count_of_records']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v22.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the row that contains the null values\n",
    "biblio_subjectUniqueTerms_v23 = biblio_subjectUniqueTerms_v22[biblio_subjectUniqueTerms_v22['subfield_a'] != 'null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v23.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v23.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v24 = biblio_subjectUniqueTerms_v23.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v24.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when is there more than 1 value?\n",
    "\n",
    "biblio_subjects_Strings_wrong = biblio_subjectUniqueTerms_v24[biblio_subjectUniqueTerms_v24['unique_ids'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjects_Strings_wrong.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(biblio_subjects_Strings_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many records have the problem?\n",
    "records_wrong_string = biblio_subjectUniqueTerms_v23['ids'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "records_wrong_string[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "### Subjects with same Id and multiple strings (toDo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "#### Example record with both problems (no 1-1 relationship subject string/Id)\n",
    "(0 and a should be 1 to 1)\n",
    "1) why are the unique counts different? are the identifiers from 650-0 added only when a term is entered in Authorities? why are there terms without field 650-0?\n",
    "2) In the test term below, why do the inconsistencies occur? I noticed that if one picks up a term from authorities, both the id and the string are loaded, but the string can be changed. -> shouldn't we try to lock the edit in Biblio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to test why there is no one-to-one correspondence between the string and the id in the correspondent authority\n",
    "test_term2 = biblio_650_df[biblio_650_df['indicator_0'].str.contains(\"318765\", case=False, regex=True)] # nuclear weapons\n",
    "# test_term2 = biblio_650_df[biblio_650_df['indicator_0'].str.contains(\"370805\", case=False, regex=True)]\n",
    "\n",
    "test_term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_term_strings = test_term2['indicator_0'].unique().tolist()\n",
    "for term in test_term_strings:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_term_strings = test_term2['subfield_a'].unique().tolist()\n",
    "for term in test_term_strings:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by LOCAL id (subjects_with_local_ids only)\n",
    "biblio_subjectUniqueTerms_v60 = subjects_with_local_ids.groupby('indicator_0', as_index=False).agg(\n",
    "    count_of_records=('001', 'nunique'),\n",
    "    ids=('001', lambda x: ','.join(map(str, x))),\n",
    "    unique_strings =('subfield_a', lambda x: len(set(x.dropna()))),\n",
    "    strings=('subfield_a', lambda x: ','.join(sorted(set(x.dropna()))))\n",
    ")\n",
    "\n",
    "\n",
    "# Sort by 'count_of_ids' in descending order (most frequent to least frequent)\n",
    "biblio_subjectUniqueTerms_v61 = biblio_subjectUniqueTerms_v60.sort_values(by='count_of_records', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v61.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v64 = biblio_subjectUniqueTerms_v61[biblio_subjectUniqueTerms_v61['unique_strings'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v65 = biblio_subjectUniqueTerms_v64.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v65['indicator_0'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all 'ids' into one big string, split by commas, and count unique values\n",
    "total_unique_ids = len(set(\n",
    "    ','.join(biblio_subjectUniqueTerms_v65['ids'].dropna()).split(',')\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblio_subjectUniqueTerms_v65['unique_ids_count'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "showdf = biblio_subjectUniqueTerms_v65[['indicator_0', 'count_of_records', 'strings', 'unique_strings']]\n",
    "showdf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "# AUTHORITIES subject terms\n",
    "- These terms are extracted from the IISH metadata using the public version of the OAI-PMH endpoint. For more information about what AUTHORITIES contains, see: https://confluence.socialhistoryservices.org/x/S4FeBw.\n",
    "- The harvesting was done using the code from the \"Metadata overviews\" repository: https://github.com/lilimelgar/iisg-metadata-overviews\n",
    "- The harvesting date was November 13th, 2024.\n",
    "- Using another jupyter notebook (https://github.com/lilimelgar/iisg-metadata-overviews/blob/main/biblio/src/biblio_query.ipynb) I created a slice of the entire metadata selecting only the MARC fields that start with 6, because 600 corresponds to the group of subject terms in MARC (https://www.loc.gov/marc/bibliographic/bd6xx.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as dataframe\n",
    "authorities_subjectTerms_df_v0 = pd.read_csv(f'{data_downloads_authority}/subjects_authority_150_subfields_20250929-075346.csv', sep=\",\", low_memory=False)\n",
    "# low_memory=False was set after this warning message: \"/var/folders/3y/xbjxw0b94jxg6x2bcbyjsmmcgvnf7q/T/ipykernel_987/2912965462.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\"\n",
    "\n",
    "# history\n",
    "# authorities_subjectTerms_df_v0 = pd.read_csv(f'{data_downloads_authority}/subjects_authority_150_subfields_20250421-172541.csv', sep=\",\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "authorities_subjectTerms_df_v0.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "df_columns = authorities_subjectTerms_df_v0.columns\n",
    "for column in df_columns:\n",
    "    dataType = authorities_subjectTerms_df_v0.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].fillna('null')\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].fillna('null')\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].astype(str)\n",
    "    if dataType == object:\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].fillna('null')\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert id to string\n",
    "authorities_subjectTerms_df_v0['001'] = authorities_subjectTerms_df_v0['001'].astype(str)\n",
    "authorities_subj_df_v1 = authorities_subjectTerms_df_v0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "authorities_subj_df_v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "authorities_subj_df_v1.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "authorities_subj_df_v1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test12 = authorities_subj_df_v1[authorities_subj_df_v1['subfield_a'].str.contains(\"\", case=False, regex=True)] #\n",
    "query_test12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test_venn = authorities_subj_df_v1[authorities_subj_df_v1['subfield_a'].str.contains(\"Recruitment of personnel\", case=False, regex=True)] #\n",
    "query_test_venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMPORARILY DROP THE OUTLIER\n",
    "# authorities_sub_df_v2 = authorities_subj_df_v1.drop(300).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorities_sub_df_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_test13 = authorities_sub_df_v2[authorities_sub_df_v2['150'].str.contains('\"a\":', case=False, regex=True)] #\n",
    "# query_test13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_test13.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorities_sub_df_v2['150a'] = authorities_sub_df_v2['150'].map(lambda x: x.lstrip('\"a\":').rstrip(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorities_subj_df['150a'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_subj_df = authorities_subj_df_v1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_subj_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_subj_df.iloc[707] #Labour market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_subj_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_record = authorities_subj_df[authorities_subj_df['subfield_a'].str.contains('alca', case=False, regex=True)]\n",
    "test_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_richard = authorities_subj_df[['001', 'subfield_a']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_richard.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "name_file = 'subject_terms_Authority_150a'\n",
    "\n",
    "\n",
    "# ## or download to csv\n",
    "export_richard.to_csv(f'{data_downloads_authority}/{name_file}.csv', index=False) # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169",
   "metadata": {},
   "source": [
    "## normalize strings for venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize strings\n",
    "authorities_subj_df['concept_string_normalized'] = (authorities_subj_df['subfield_a'].str.replace('-', ' ', regex=False).copy()   # Remove dashes\n",
    "                                                                                     .str.title().copy()                         # Convert to title case\n",
    "                                                                                     .str.replace(' ', '', regex=False).copy()  # Remove spaces\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test_venn2 = authorities_subj_df[authorities_subj_df['subfield_a'].str.contains(\"Typists\", case=False, regex=True)] #\n",
    "query_test_venn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "authority_list = authorities_subj_df['concept_string_normalized'].tolist()\n",
    "# authority_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "## Get Ids for venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "authority_list_ids = authorities_subj_df_v1['001'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(authority_list_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "177",
   "metadata": {},
   "source": [
    "# POOLPARTY-THESAURUS\n",
    "Used export functionality from Poolparty to export to RDF-NQ. Then I adapted some columns by using Open Refine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as dataframe\n",
    "# poolparty_thes_df_v0 = pd.read_csv(f'{data_directory_subjects}/poolparty-thesaurus/pp-project-socialhistorytaxonomy-nq_STRINGS_ONLY_english.csv', sep=\",\", low_memory=False)\n",
    "# low_memory=False was set after this warning message: \"/var/folders/3y/xbjxw0b94jxg6x2bcbyjsmmcgvnf7q/T/ipykernel_987/2912965462.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\"\n",
    "\n",
    "poolparty_thes_df_v0 = pd.read_csv(f'{data_directory_subjects}/poolparty-thesaurus/pp-project-socialhistorytaxonomy-nq_BasicColumns.csv', sep=\",\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179",
   "metadata": {},
   "outputs": [],
   "source": [
    "poolparty_thes_df_v0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "poorparty_thes_df_v1 = poolparty_thes_df_v0.rename(columns={\"subject\": \"subjectId\", \"http://www.w3.org/2004/02/skos/core#prefLabel\": \"prefLabel\", \"concept_string_normalized\": \"concept_string_normalized\", \"concept_string_language\": \"concept_string_language\", \"http://www.w3.org/2004/02/skos/core#notation\": \"authorityId\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "poorparty_thes_df_v1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "pp_columns = poorparty_thes_df_v1.columns\n",
    "for column in pp_columns:\n",
    "    dataType = poorparty_thes_df_v1.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        poorparty_thes_df_v1[column] = poorparty_thes_df_v1[column].fillna('null')\n",
    "        poorparty_thes_df_v1[column] = poorparty_thes_df_v1[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        poorparty_thes_df_v1[column] = poorparty_thes_df_v1[column].fillna('null')\n",
    "        poorparty_thes_df_v1[column] = poorparty_thes_df_v1[column].astype(str)\n",
    "    if dataType == object:\n",
    "        poorparty_thes_df_v1[column] = poorparty_thes_df_v1[column].fillna('null')\n",
    "        poorparty_thes_df_v1[column] = poorparty_thes_df_v1[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "poorparty_thes_df_v1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "poorparty_thes_df = poorparty_thes_df_v1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {},
   "source": [
    "## normalize strings and Ids for venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "poolparty_thes_list = poorparty_thes_df['concept_string_normalized'].tolist()\n",
    "poolparty_thes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(poolparty_thes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "poolparty_authority_ids_notnull = poorparty_thes_df[poorparty_thes_df['authorityId'] != 'null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poolparty Id list\n",
    "poolparty_authority_ids = poolparty_authority_ids_notnull['authorityId'].tolist()\n",
    "poolparty_authority_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poolparty_authority_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191",
   "metadata": {},
   "source": [
    "# PAPER-THESAURUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as dataframe\n",
    "paper_thesaurus_df_v0 = pd.read_csv(f'{data_directory_subjects}/paper-thesaurus/iish-thesaurus-pdf_STRINGS_ONLY.csv', sep=\",\", low_memory=False)\n",
    "# low_memory=False was set after this warning message: \"/var/folders/3y/xbjxw0b94jxg6x2bcbyjsmmcgvnf7q/T/ipykernel_987/2912965462.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_thesaurus_df_v0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_thes_list = paper_thesaurus_df_v0['concept_string_normalized'].tolist()\n",
    "paper_thes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "199",
   "metadata": {},
   "source": [
    "# COMPARISONS (VENN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200",
   "metadata": {},
   "source": [
    "## Comparing Poolparty-thesaurus to Paper-thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install matplotlib-venn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sets\n",
    "set1 = set(paper_thes_list)\n",
    "set2 = set(poolparty_thes_list)\n",
    "\n",
    "# venn2(subsets = (3, 2, 1))\n",
    "\n",
    "# venn2((set(['A', 'B', 'C', 'D']), set(['D', 'E', 'F'])))\n",
    "venn2([set1, set2], set_labels=('Paper-thesaurus', 'Poolparty-thesaurus'))\n",
    "plt.title(\"Venn Diagram of Paper-thesaurus and Poolparty-thesaurus\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this roughly indicates that from the 2080 terms in Poolparty, 1062 match with the paper thesaurus\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "# Your sets\n",
    "venn = venn2([set1, set2], set_labels=('Paper-thesaurus', 'Poolparty-thesaurus'))\n",
    "\n",
    "# Custom colors\n",
    "venn.get_patch_by_id('10').set_color('#66c2a5')  # Paper-only\n",
    "venn.get_patch_by_id('01').set_color('#fc8d62')  # Poolparty-only\n",
    "venn.get_patch_by_id('11').set_color('#8da0cb')  # Shared\n",
    "\n",
    "# Optional: transparency\n",
    "for subset in ('10', '01', '11'):\n",
    "    patch = venn.get_patch_by_id(subset)\n",
    "    if patch:\n",
    "        patch.set_alpha(0.7)\n",
    "\n",
    "# Title\n",
    "plt.title(\"Venn Diagram of Paper-thesaurus and Poolparty-thesaurus\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "# Your sets\n",
    "venn = venn2([set1, set2], set_labels=('Paper-thesaurus', 'Poolparty-thesaurus'))\n",
    "\n",
    "# Set custom colors from the Lucid palette\n",
    "colors = {\n",
    "    '10': '#85bdff',  # Paper only\n",
    "    '01': '#FF6F91',  # Poolparty only\n",
    "    '11': '#845EC2',  # Overlap\n",
    "}\n",
    "\n",
    "# Apply colors\n",
    "for subset_id, color in colors.items():\n",
    "    patch = venn.get_patch_by_id(subset_id)\n",
    "    if patch:\n",
    "        patch.set_color(color)\n",
    "        patch.set_alpha(0.35)\n",
    "\n",
    "# Optional: white text and clearer labels\n",
    "for text in venn.set_labels:\n",
    "    text.set_fontsize(12)\n",
    "    text.set_color('black')\n",
    "\n",
    "plt.title(\"Venn Diagram of Paper-thesaurus and Poolparty-thesaurus\", fontsize=14)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib_venn import venn3\n",
    "\n",
    "# set1 = set(['A', 'B', 'C', 'D'])\n",
    "# set2 = set(['B', 'C', 'D', 'E'])\n",
    "# set3 = set(['C', 'D',' E', 'F', 'G'])\n",
    "\n",
    "# venn3([set1, set2, set3], ('Set1', 'Set2', 'Set3'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set difference: items in authority_list but not in poolparty_thes_list\n",
    "non_matching = set1 - set2  # or: set1.difference(set2)\n",
    "\n",
    "# Print all (optional)\n",
    "print(\"Terms in Paper-thesaurus not in poolparty_thes_list:\")\n",
    "print(non_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set difference: items in authority_list but not in poolparty_thes_list\n",
    "non_matching = set2 - set1  # or: set1.difference(set2)\n",
    "\n",
    "# Print all (optional)\n",
    "print(\"Terms in Poolparty_thes_list not in Paper-thesaurus:\")\n",
    "print(non_matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209",
   "metadata": {},
   "source": [
    "## Comparing Authority subject terms & Poolparty-thesaurus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210",
   "metadata": {},
   "source": [
    "### Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sets\n",
    "set1 = set(authority_list)\n",
    "set2 = set(poolparty_thes_list)\n",
    "\n",
    "# venn2(subsets = (3, 2, 1))\n",
    "\n",
    "# # venn2((set(['A', 'B', 'C', 'D']), set(['D', 'E', 'F'])))\n",
    "# venn2([set1, set2], set_labels=('Authority subject terms', 'Poolparty-thesaurus'))\n",
    "# plt.title(\"Venn Diagram of Authority subject terms and Poolparty-thesaurus\")\n",
    "# plt.show()\n",
    "\n",
    "# Your sets\n",
    "venn = venn2([set1, set2], set_labels=('Authority subjects', 'Poolparty-thesaurus'))\n",
    "\n",
    "# Set custom colors from the Lucid palette\n",
    "colors = {\n",
    "    '10': '#85bdff',  # Paper only\n",
    "    '01': '#FF6F91',  # Poolparty only\n",
    "    '11': '#845EC2',  # Overlap\n",
    "}\n",
    "\n",
    "# Apply colors\n",
    "for subset_id, color in colors.items():\n",
    "    patch = venn.get_patch_by_id(subset_id)\n",
    "    if patch:\n",
    "        patch.set_color(color)\n",
    "        patch.set_alpha(0.35)\n",
    "\n",
    "# Optional: white text and clearer labels\n",
    "for text in venn.set_labels:\n",
    "    text.set_fontsize(12)\n",
    "    text.set_color('black')\n",
    "\n",
    "\n",
    "plt.title(\"Venn Diagram of Authority subjects and Poolparty-thesaurus\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set difference: items in authority_list but not in poolparty_thes_list\n",
    "non_matching = set1 - set2  # or: set1.difference(set2)\n",
    "\n",
    "# Print all (optional)\n",
    "print(\"Terms in authority_list not in poolparty_thes_list:\")\n",
    "print(non_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set difference: items in authority_list but not in poolparty_thes_list\n",
    "non_matching = set2 - set1  # or: set1.difference(set2)\n",
    "\n",
    "# Print all (optional)\n",
    "print(\"Terms in Poolparty not in Authority:\")\n",
    "print(non_matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214",
   "metadata": {},
   "source": [
    "### Ids from Authorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sets\n",
    "set1 = set(authority_list_ids)\n",
    "set2 = set(poolparty_authority_ids)\n",
    "\n",
    "# venn2(subsets = (3, 2, 1))\n",
    "\n",
    "# # venn2((set(['A', 'B', 'C', 'D']), set(['D', 'E', 'F'])))\n",
    "# venn2([set1, set2], set_labels=('Authority subject terms', 'Poolparty-thesaurus'))\n",
    "# plt.title(\"Venn Diagram of Authority subject terms and Poolparty-thesaurus\")\n",
    "# plt.show()\n",
    "\n",
    "# Your sets\n",
    "venn = venn2([set1, set2], set_labels=('Authority subject Ids', 'Poolparty-thesaurus Ids'))\n",
    "\n",
    "# Set custom colors from the Lucid palette\n",
    "colors = {\n",
    "    '10': '#85bdff',  # AuthorityIds\n",
    "    '01': '#FF6F91',  # Poolparty AuthorityIds\n",
    "    '11': '#845EC2',  # Overlap\n",
    "}\n",
    "\n",
    "# Apply colors\n",
    "for subset_id, color in colors.items():\n",
    "    patch = venn.get_patch_by_id(subset_id)\n",
    "    if patch:\n",
    "        patch.set_color(color)\n",
    "        patch.set_alpha(0.35)\n",
    "\n",
    "# Optional: white text and clearer labels\n",
    "for text in venn.set_labels:\n",
    "    text.set_fontsize(12)\n",
    "    text.set_color('black')\n",
    "\n",
    "\n",
    "plt.title(\"Venn Diagram of Authority subject Ids and Poolparty-thesaurus Authority Ids\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216",
   "metadata": {},
   "source": [
    "## Comparing biblio unique terms with authorities unique terms (strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sets\n",
    "set1 = set(authority_list)\n",
    "set2 = set(biblio_subjects_list)\n",
    "\n",
    "# venn2(subsets = (3, 2, 1))\n",
    "\n",
    "# # venn2((set(['A', 'B', 'C', 'D']), set(['D', 'E', 'F'])))\n",
    "# venn2([set1, set2], set_labels=('Authority subject terms', 'Biblio subject terms'))\n",
    "# plt.title(\"Venn Diagram of Authority subject terms and Biblio subject terms\")\n",
    "# plt.show()\n",
    "\n",
    "# Your sets\n",
    "venn = venn2([set1, set2], set_labels=('Authority subjects', 'Biblio subject terms'))\n",
    "\n",
    "# Set custom colors from the Lucid palette\n",
    "colors = {\n",
    "    '10': '#85bdff',  # Paper only\n",
    "    '01': '#FF6F91',  # Poolparty only\n",
    "    '11': '#845EC2',  # Overlap\n",
    "}\n",
    "\n",
    "# Apply colors\n",
    "for subset_id, color in colors.items():\n",
    "    patch = venn.get_patch_by_id(subset_id)\n",
    "    if patch:\n",
    "        patch.set_color(color)\n",
    "        patch.set_alpha(0.35)\n",
    "\n",
    "# Optional: white text and clearer labels\n",
    "for text in venn.set_labels:\n",
    "    text.set_fontsize(12)\n",
    "    text.set_color('black')\n",
    "\n",
    "\n",
    "plt.title(\"Venn Diagram of Authority subject terms and Biblio subject terms\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set difference: items in authority_list but not in poolparty_thes_list\n",
    "non_matching = set1 - set2  # or: set1.difference(set2)\n",
    "\n",
    "# Print all (optional)\n",
    "print(\"Terms in authority_list not in biblio_list:\")\n",
    "print(non_matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_650_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220",
   "metadata": {},
   "source": [
    "#### compare the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get string column from Biblio\n",
    "biblio_subject_strings_df_v0 = biblio_650_df[['001','subfield_a']]\n",
    "biblio_subject_strings_df_v0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subject_strings_df_v0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subject_strings_df_v1 = biblio_subject_strings_df_v0[biblio_subject_strings_df_v0['subfield_a'] != 'null']\n",
    "biblio_subject_strings_df_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subject_strings_df = biblio_subject_strings_df_v1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subject_strings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_subj_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get string column from Authority\n",
    "authority_subject_strings_df_v0 = authorities_subj_df[['001','subfield_a']]\n",
    "authority_subject_strings_df_v0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228",
   "metadata": {},
   "outputs": [],
   "source": [
    "authority_subject_strings_df = authority_subject_strings_df_v0.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare strings\n",
    "# biblio_subject_strings_df['stringBiblio_in_Authority'] = biblio_subject_strings_df['subfield_a'].isin(authority_subject_strings_df['subfield_a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# biblio_subject_strings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename A's column for clarity during merge\n",
    "authority_subject_strings_df_renamed = authority_subject_strings_df[['subfield_a']].rename(columns={'subfield_a': 'string_match'})\n",
    "\n",
    "# Merge B with A based on matching strings\n",
    "comparison_df = biblio_subject_strings_df.merge(\n",
    "    authority_subject_strings_df_renamed,\n",
    "    how='left',\n",
    "    left_on='subfield_a',\n",
    "    right_on='string_match'\n",
    ")\n",
    "\n",
    "# Add a boolean 'Matches' column\n",
    "comparison_df['Matches'] = comparison_df['string_match'].notna()\n",
    "\n",
    "\n",
    "comparison_df.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df_unique = comparison_df.drop_duplicates()\n",
    "comparison_df_unique.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234",
   "metadata": {},
   "outputs": [],
   "source": [
    "test20 = comparison_df_unique[comparison_df_unique['subfield_a'].str.contains(\"Refugees*\", case=False, regex=True)]\n",
    "test20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df['001'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236",
   "metadata": {},
   "source": [
    "## Comparing biblio unique terms with authorities unique terms (Ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to sets\n",
    "set1 = set(authority_list_ids)\n",
    "set2 = set(biblio_subjects_Ids_list)\n",
    "\n",
    "# venn2(subsets = (3, 2, 1))\n",
    "\n",
    "# # venn2((set(['A', 'B', 'C', 'D']), set(['D', 'E', 'F'])))\n",
    "# venn2([set1, set2], set_labels=('Authority subject terms (ids)', 'Biblio subject terms (ids)'))\n",
    "# plt.title(\"Venn Diagram of Authority subject terms (ids) and Biblio subject terms (ids)\")\n",
    "# plt.show()\n",
    "\n",
    "# Your sets\n",
    "venn = venn2([set1, set2], set_labels=('Authority subject terms Ids', 'Biblio subject terms Ids'))\n",
    "\n",
    "# Set custom colors from the Lucid palette\n",
    "colors = {\n",
    "    '10': '#85bdff',  # Paper only\n",
    "    '01': '#FF6F91',  # Poolparty only\n",
    "    '11': '#845EC2',  # Overlap\n",
    "}\n",
    "\n",
    "# Apply colors\n",
    "for subset_id, color in colors.items():\n",
    "    patch = venn.get_patch_by_id(subset_id)\n",
    "    if patch:\n",
    "        patch.set_color(color)\n",
    "        patch.set_alpha(0.35)\n",
    "\n",
    "# Optional: white text and clearer labels\n",
    "for text in venn.set_labels:\n",
    "    text.set_fontsize(12)\n",
    "    text.set_color('black')\n",
    "\n",
    "\n",
    "plt.title(\"Venn Diagram of Authority subject terms (ids) and Biblio subject terms (ids)\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set difference: items in authority_list but not in poolparty_thes_list\n",
    "non_matching_1 = set1 - set2  # or: set1.difference(set2)\n",
    "\n",
    "# Print all (optional)\n",
    "print(\"Terms in authority_list NOT in biblio_list:\")\n",
    "print(non_matching_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(non_matching_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set difference: items in authority_list but not in poolparty_thes_list\n",
    "non_matching_2 = set2 - set1  # or: set1.difference(set2)\n",
    "\n",
    "# Print all (optional)\n",
    "print(\"Terms in biblio_list NOT in authority_list:\")\n",
    "print(non_matching_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "242",
   "metadata": {},
   "source": [
    "## Comparing main three (for updating thesaurus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # set4 = set(biblio_subjects_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example sets (replace with yours)\n",
    "set1 = set(paper_thes_list)\n",
    "set2 = set(poolparty_thes_list)\n",
    "set3 = set(authority_list)\n",
    "\n",
    "# Create Venn diagram\n",
    "venn = venn3([set1, set2, set3],\n",
    "             set_labels=('Paper thes. strings', 'Poolparty strings', 'Authority strings'))\n",
    "\n",
    "# Custom color mapping (you can tweak as needed)\n",
    "colors = {\n",
    "    '100': '#85bdff',   # Authority only\n",
    "    '010': '#FF6F91',   # Biblio only\n",
    "    '001': '#FFC75F',   # PoolParty only\n",
    "    '110': '#A1D5FF',   # Authority  Biblio\n",
    "    '101': '#B39CD0',   # Authority  PoolParty\n",
    "    '011': '#F9F871',   # Biblio  PoolParty\n",
    "    '111': '#845EC2',   # All three\n",
    "}\n",
    "\n",
    "# Apply colors\n",
    "for subset_id, color in colors.items():\n",
    "    patch = venn.get_patch_by_id(subset_id)\n",
    "    if patch:\n",
    "        patch.set_color(color)\n",
    "        patch.set_alpha(0.5)\n",
    "\n",
    "# Optional label tweaks\n",
    "for text in venn.set_labels:\n",
    "    if text:\n",
    "        text.set_fontsize(11)\n",
    "        text.set_color('black')\n",
    "\n",
    "plt.title(\"Venn Diagram: Authority, \", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "246",
   "metadata": {},
   "source": [
    "# (not used now) MAPPINGS authority - Poolparty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247",
   "metadata": {},
   "source": [
    "ListA = authorities_sub_df\n",
    "ListB = poolparty_df\n",
    "\n",
    "<!-- Authorities -->\n",
    "<!-- #   Column  Non-Null Count  Dtype \n",
    "---  ------  --------------  ----- \n",
    " 0   001     982 non-null    object\n",
    " 1   150     982 non-null    object\n",
    " 2   450     982 non-null    object\n",
    " 3   550     982 non-null    object\n",
    " 4   leader  982 non-null    object\n",
    " 5   150a    982 non-null    object -->\n",
    "\n",
    "\n",
    "<!-- Poolparty -->\n",
    "<!-- #   Column             Non-Null Count  Dtype \n",
    "---  ------             --------------  ----- \n",
    " 0   Concept            2213 non-null   object\n",
    " 1   PreferredLabels    2213 non-null   object\n",
    " 2   AlternativeLabels  2213 non-null   object\n",
    " 3   BroaderConcepts    2213 non-null   object\n",
    " 4   NarrowerConcepts   2213 non-null   object -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This python script detects the string similarity between two lists of concepts/terms.\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def compare_strings(dfA, dfB):\n",
    "    '''Processes and maps candidate names\n",
    "    Inputs are two dataframes of names\n",
    "    Outputs a dataframe of candidates\n",
    "    '''\n",
    "\n",
    "    # # create an empty dataframe\n",
    "    # mapped_candidates_df = pd.DataFrame()\n",
    "    ##COLLECT ROWS\n",
    "    rows = []\n",
    "\n",
    "    ############################## CAPTURE VARIABLES FROM DFs #######################################\n",
    "    # for indexB, rowB in dfB.iterrows():\n",
    "    for indexB, rowB in tqdm(dfB.iterrows(), total=dfB.shape[0]):\n",
    "        # Capture basic standard columns for the mapping dataset B (to be mapped) as variables\n",
    "        idB = dfB.loc[indexB, 'Concept']\n",
    "        stringB = dfB.loc[indexB, 'PreferredLabels']\n",
    "        alternativeLabelsB = dfB.loc[indexB, 'AlternativeLabels']\n",
    "        broaderConceptsB = dfB.loc[indexB, 'BroaderConcepts']\n",
    "        narrowerConceptsB = dfB.loc[indexB, 'NarrowerConcepts']\n",
    "        sleep(0.01)\n",
    "        for indexA, rowA in dfA.iterrows():\n",
    "            # Capture basic standard columns for the mapping dataset A (to be mapped to) as variables\n",
    "            idA = dfA.loc[indexA, '001']\n",
    "            stringA = dfA.loc[indexA, '150a']\n",
    "            marc150 = dfA.loc[indexA, '150']\n",
    "            marc450 = dfA.loc[indexA, '450']\n",
    "            marc550 = dfA.loc[indexA, '550']        \n",
    "            leader = dfA.loc[indexA, 'leader']\n",
    "\n",
    "    ############################## SET STRING MATCHING SETTINGS #######################################\n",
    "\n",
    "            # Algorithm to be used\n",
    "            matchScore1 = fuzz.token_sort_ratio(stringA.lower(), stringB.lower())\n",
    "            # matchScore2 = fuzz.token_set_ratio(stringA, stringB)\n",
    "            # matchScore3 = fuzz.partial_ratio(nameStringA, nameStringB) # USE WITH casesNoisy (edit below) if names in both datasets are very similar. It compares parts of strings, low score is useful to avoid matches like this (('Carlieri Jacopo', 'Jacopo Battieri'))\n",
    "\n",
    "            # String score ranges\n",
    "            rangeScoreVeryLow = 80\n",
    "            rangeScoreLow = 85\n",
    "            rangeScoreMid = 90\n",
    "            rangeScoreHigh = 100\n",
    "\n",
    "\n",
    "    # ############################## RUN STRING MATCHING #######################################\n",
    "            # this rule only applies to cases of type A when the dates are exactly the same (e.g., to match 'Olivarius Vredius' with 'Olivier de Wree')\n",
    "            if rangeScoreVeryLow <= matchScore1 <= rangeScoreHigh:\n",
    "                scoreString = dfA.loc[indexA,'scoreString'] = matchScore1\n",
    "                scoreType = dfA.loc[indexA, 'scoreType'] = 'matchScore1'\n",
    "                match_StringB = dfA.loc[indexA,'match-stringB'] = stringB\n",
    "                match_idB = idB = dfA.loc[indexA,'match-idB'] = idB\n",
    "                rows.append({\n",
    "                    'scoreString': scoreString,\n",
    "                    'scoreType': scoreType,\n",
    "                    'idA':idA,                    \n",
    "                    'match_idB': match_idB,\n",
    "                    'stringA': stringA,                    \n",
    "                    'match_stringB': match_StringB,\n",
    "                    'marc150': marc150, \n",
    "                    'marc450': marc450, \n",
    "                    'marc550': marc550, \n",
    "                    'leader': leader,\n",
    "                    'alternativeLabelsB': alternativeLabelsB, \n",
    "                    'broaderConceptsB': broaderConceptsB, \n",
    "                    'narrowerConceptsB': narrowerConceptsB\n",
    "                })\n",
    "\n",
    "        df_mapped = pd.DataFrame(rows)\n",
    "\n",
    "    return df_mapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfA = authorities_sub_df\n",
    "# dfB = poolparty_df\n",
    "\n",
    "mapped_candidates = compare_strings(authorities_sub_df, poolparty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_candidates.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252",
   "metadata": {},
   "outputs": [],
   "source": [
    "test12 = mapped_candidates[mapped_candidates['StringA'].str.contains(\"strike*|staking*\", case=False, regex=True)]\n",
    "test12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how many terms are in Biblio that have a correspondent Authority record?\n",
    "# biblio_subjectTerms_notnull = biblio_subjectTerms_df[biblio_subjectTerms_df['650'] == 'notnull']\n",
    "# question2 = biblio_subjectTerms_notnull[biblio_subjectTerms_notnull['\"0\"'].str.contains(\"NL-AMISG)\", case=True, regex=False)]\n",
    "# question2_total = question2.shape[0]\n",
    "# question2_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how many terms are in Biblio that do not have a correspondent Authority record?\n",
    "# biblio_subjectTerms_a_notnull = biblio_subjectTerms_df[biblio_subjectTerms_df['\"a\"'] is not ]\n",
    "# question3 = biblio_subjectTerms_notnull[~biblio_subjectTerms_notnull['\"0\"'].str.contains(\"\", case=True, regex=False)]\n",
    "# question3_total = biblio_subjectTerms_null.shape[0]\n",
    "# question3_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# questionTest = biblio_subjectTerms_df['\"0\"'].value_counts()\n",
    "# questionTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_test = subject_terms_df[\n",
    "#     subject_terms_df['\"a\"'].str.contains(\"collective\", case=False, regex=True) & \n",
    "#     subject_terms_df[\"another_column\"].str.contains(\"stringTest\", case=False, regex=True)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_terms_df['\"x\"'].unique()\n",
    "# query_test = subject_terms_df[subject_terms_df['\"a\"'].str.contains(\"collective\", case=False, regex=True)] #\n",
    "# query_test\n",
    "# query_test['\"a\"'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
