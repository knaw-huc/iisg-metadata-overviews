{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Jupyter notebook to get an overview of the different subject terms used at the IISH from four different sources\n",
    "- dateCreated: 2025-01-06\n",
    "- creator: Liliana Melgar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# import os.path to add paths to files\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Set paths to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to where the relevant data is located\n",
    "# biblio\n",
    "script_dir = os.getcwd()  # Gets the current working directory\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\", \"..\"))  # Moves up two levels to reach 'repo'\n",
    "data_directory_biblio = os.path.join(project_root, \"data\", \"biblio\")\n",
    "data_downloads_biblio = os.path.join(data_directory_biblio, 'downloads') #path to the folder where the reports will be downloaded\n",
    "\n",
    "# authority\n",
    "script_dir = os.getcwd()  # Gets the current working directory\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\", \"..\"))  # Moves up two levels to reach 'repo'\n",
    "data_directory_authority = os.path.join(project_root, \"data\", \"authority\")\n",
    "data_downloads_authority = os.path.join(data_directory_authority, 'downloads') #path to the folder where the reports will be downloaded\n",
    "\n",
    "# subjects (thesauri)\n",
    "script_dir = os.getcwd()  # Gets the current working directory\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\", \"..\"))  # Moves up two levels to reach 'repo'\n",
    "data_directory_subjects = os.path.join(project_root, \"data\", \"subjects\")\n",
    "data_downloads_subjects = os.path.join(data_directory_subjects, 'downloads') #path to the folder where the reports will be downloaded\n",
    "\n",
    "# persons\n",
    "script_dir = os.getcwd()  # Gets the current working directory\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\", \"..\"))  # Moves up two levels to reach 'repo'\n",
    "data_directory_persons = os.path.join(project_root, \"data\", \"persons\")\n",
    "data_downloads_persons = os.path.join(data_directory_subjects, 'downloads') #path to the folder where the reports will be downloaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# BIBLIO Persons\n",
    "- These terms are extracted from the IISH metadata using the public version of the OAI-PMH endpoint. For more information about what BIBLIO contains, see: https://confluence.socialhistoryservices.org/x/S4FeBw.\n",
    "- The harvesting was done using the code from the \"Metadata overviews\" repository: https://github.com/lilimelgar/iisg-metadata-overviews\n",
    "- **The harvesting date was April 4, 2025**.\n",
    "- Using another jupyter notebook (https://github.com/lilimelgar/iisg-metadata-overviews/blob/main/biblio/src/biblio_query.ipynb) I created a slice of the entire metadata selecting only the MARC field 100, because 100 corresponds to the persons/authors/creators in MARC (https://www.loc.gov/marc/bibliographic/bd100.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as dataframe\n",
    "\n",
    "biblio_persons_df_v0 = pd.read_csv(f'{data_downloads_biblio}/persons_100_subfields.csv', sep=\",\", low_memory=False)\n",
    "\n",
    "# biblio_subjectTerms_df_v0 = pd.read_csv(f'{data_directory}/biblio_subjectTerms600.csv.gzip', sep=\",\", compression='gzip', low_memory=False)\n",
    "# low_memory=False was set after this warning message: \"/var/folders/3y/xbjxw0b94jxg6x2bcbyjsmmcgvnf7q/T/ipykernel_987/2912965462.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_persons_df_v0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "df_columns = biblio_persons_df_v0.columns\n",
    "for column in df_columns:\n",
    "    dataType = biblio_persons_df_v0.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        biblio_persons_df_v0[column] = biblio_persons_df_v0[column].fillna('null')\n",
    "        biblio_persons_df_v0[column] = biblio_persons_df_v0[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        biblio_persons_df_v0[column] = biblio_persons_df_v0[column].fillna('null')\n",
    "        biblio_persons_df_v0[column] = biblio_persons_df_v0[column].astype(str)\n",
    "    if dataType == object:\n",
    "        biblio_persons_df_v0[column] = biblio_persons_df_v0[column].fillna('null')\n",
    "        biblio_persons_df_v0[column] = biblio_persons_df_v0[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "biblio_persons_df = biblio_persons_df_v0.copy()\n",
    "biblio_persons_df.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "biblio_persons_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "biblio_persons_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_persons_df['001'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Records with persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subset of biblio with relevant columns\n",
    "biblio_persons_df_v0 = biblio_persons_df[['001','100', 'leader_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_persons_df_v0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of 'null' and 'notNull'\n",
    "value_counts = biblio_persons_df_v0.groupby('100')['001'].nunique()\n",
    "value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total unique '001' count\n",
    "total_unique_ids = biblio_persons_df_v0['001'].nunique()\n",
    "total_unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot (Label mapping)\n",
    "labels_mapping = {\n",
    "    'notnull': f'records with field 100 ({value_counts.get(\"notnull\", 0)})',\n",
    "    'null': f'records without field 100 ({value_counts.get(\"null\", 0)})'\n",
    "}\n",
    "custom_labels = [labels_mapping[label] for label in value_counts.index]\n",
    "\n",
    "# Colors\n",
    "colors = ['#90ee90','#cccccc']\n",
    "\n",
    "# Step 4: Plot pie chart\n",
    "fig1, ax = plt.subplots(figsize=(7,6))\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    value_counts, labels=custom_labels, autopct='%1.1f%%', colors=colors, startangle=90,\n",
    "    wedgeprops={'edgecolor': 'white', 'linewidth': 2}, pctdistance=0.85\n",
    ")\n",
    "\n",
    "# Donut hole\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig1.gca().add_artist(centre_circle)\n",
    "\n",
    "# Center text\n",
    "ax.text(0, 0, f'Total number of records in Biblio\\n{total_unique_ids}', \n",
    "        ha='center', va='center', fontsize=7, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_file = 'plot1_recordsWithPerson100'\n",
    "fig1.savefig(f'{data_downloads_subjects}/{name_file}.png', format='png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### report including leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by null and notnull and leader code\n",
    "value_counts_with_leader = biblio_persons_df_v0.groupby(['100', 'leader_code'])['001'].count()\n",
    "value_counts_with_leader.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Assuming `result2` is a MultiIndex Series from groupby:\n",
    "# Convert to DataFrame and reset index\n",
    "df = value_counts_with_leader.reset_index(name='count')\n",
    "\n",
    "# Create a sunburst plot (nested pie chart)\n",
    "fig = px.sunburst(\n",
    "    df,\n",
    "    path=['100', 'leader_code'],  # First level: 'has'/'doesn't have', second level: 'code'\n",
    "    values='count',\n",
    "    title='Distribution of Records by Person Presence and Code'\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df = value_counts_with_leader.reset_index(name='count')\n",
    "\n",
    "fig = px.treemap(\n",
    "    df,\n",
    "    path=['100', 'leader_code'],  # hierarchy levels\n",
    "    values='count',\n",
    "    title='Treemap: Record Distribution by Person Presence and Code'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = value_counts_with_leader.reset_index(name='count')\n",
    "\n",
    "fig = px.bar(\n",
    "    df,\n",
    "    x='leader_code',\n",
    "    y='count',\n",
    "    color='100',\n",
    "    barmode='group',\n",
    "    title='Grouped Bar Chart: Record Counts by Code and 100'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Unique subject terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using strings\n",
    "biblio_persons_df['\"a\"'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Ids from authority\n",
    "biblio_persons_df['\"0\"'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_persons_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a term that doesn't have an identifier in 650$0\n",
    "test_term1 = biblio_persons_df[\n",
    "    (biblio_persons_df['\"0\"'].str.lower() == \"null\") &\n",
    "    (biblio_persons_df['\"a\"'].str.lower() != \"null\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_term1.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_term1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_term1['001'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_term1['001'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Questions about the number of unique subject terms: \n",
    "1) why are the unique counts different? are the identifiers from 650$0 added only when a term is entered in Authorities? why are there terms without field 650$0?\n",
    "2) In the test term below, why do the inconsistencies occur? I noticed that if one picks up a term from authorities, both the id and the string are loaded, but the string can be changed. -> shouldn't we try to lock the edit in Biblio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example to test why there is no one-to-one correspondence between the string and the id in the correspondent authority\n",
    "test_term2 = biblio_persons_df[biblio_persons_df['\"001\"'].str.contains(\"1050212\", case=False, regex=True)]\n",
    "test_term2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_term_strings = test_term2['\"0\"'].unique().tolist()\n",
    "for term in test_term_strings:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_term_strings = test_term2['\"a\"'].unique().tolist()\n",
    "for term in test_term_strings:\n",
    "    print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df groupping per term showing the counts\n",
    "# Group by 'term' and count unique '001' values for each term\n",
    "biblio_subjectUniqueTerms_v0 = biblio_subjectTerms_df.groupby('\"a\"', as_index=False).agg(\n",
    "    count_of_records=('001', 'nunique'),\n",
    "    ids=('001', lambda x: ','.join(map(str, x)))\n",
    ")\n",
    "\n",
    "# Sort by 'count_of_ids' in descending order (most frequent to least frequent)\n",
    "biblio_subjectUniqueTerms_v1 = biblio_subjectUniqueTerms_v0.sort_values(by='count_of_records', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove column with the record Ids\n",
    "biblio_subjectUniqueTerms_v2 = biblio_subjectUniqueTerms_v1[['\"a\"','count_of_records']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the row that contains the null values\n",
    "biblio_subjectUniqueTerms_v3 = biblio_subjectUniqueTerms_v2[biblio_subjectUniqueTerms_v2['\"a\"'] != 'null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v4 = biblio_subjectUniqueTerms_v3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine how many persons should be shown\n",
    "top_r = 20\n",
    "# create small df for displaying and plotting\n",
    "biblio_subjectUniqueTerms_top = biblio_subjectUniqueTerms_v4.head(top_r).reset_index(drop=True).copy()\n",
    "\n",
    "# plotting in a barh chart the top n terms\n",
    "fig2, ax = plt.subplots(figsize=(20, 10))  # Create the figure object\n",
    "ax = biblio_subjectUniqueTerms_top.groupby(['\"a\"'])['count_of_records'].sum().sort_values(ascending=True).tail(top_r).plot(kind='barh', figsize=(20, 10))\n",
    "ax.set_title(\"Top terms in Biblio's 650$a field\")\n",
    "ax.set_xlabel(\"Number biblio records\")\n",
    "ax.set_ylabel(\"Term $a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_file = 'plot2_uniqueSubjectTerms650a--'\n",
    "# Save the figure as PNG\n",
    "fig2.savefig(f'{data_downloads_subjects}/{name_file}.png', format='png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export all list of unique subject terms with the number of occurrences in Biblio\n",
    "biblio_subjectUniqueTerms_v1.rename(columns={'\"a\"': '650a', 'count_of_records': 'count_records_biblio', 'ids': 'biblio_record_ids'}, inplace=True)\n",
    "\n",
    "name_file = 'unique_650a_with_counts_and_recordIds'\n",
    "\n",
    "biblio_subjectUniqueTerms_v1.to_csv(f'{data_downloads_subjects}/{name_file}.csv', index=False) # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms_v1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms = biblio_subjectUniqueTerms_v1.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_unique_terms_biblio = biblio_subjectUniqueTerms_v1['650a'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_unique_terms_biblio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_subjectUniqueTerms.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "# AUTHORITIES subject terms\n",
    "- These terms are extracted from the IISH metadata using the public version of the OAI-PMH endpoint. For more information about what AUTHORITIES contains, see: https://confluence.socialhistoryservices.org/x/S4FeBw.\n",
    "- The harvesting was done using the code from the \"Metadata overviews\" repository: https://github.com/lilimelgar/iisg-metadata-overviews\n",
    "- The harvesting date was November 13th, 2024.\n",
    "- Using another jupyter notebook (https://github.com/lilimelgar/iisg-metadata-overviews/blob/main/biblio/src/biblio_query.ipynb) I created a slice of the entire metadata selecting only the MARC fields that start with 6, because 600 corresponds to the group of subject terms in MARC (https://www.loc.gov/marc/bibliographic/bd6xx.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as dataframe\n",
    "authorities_subjectTerms_df_v0 = pd.read_csv(f'{data_downloads_authority}/subject_terms_per_150.csv', sep=\",\", low_memory=False)\n",
    "# low_memory=False was set after this warning message: \"/var/folders/3y/xbjxw0b94jxg6x2bcbyjsmmcgvnf7q/T/ipykernel_987/2912965462.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "authorities_subjectTerms_df_v0.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "df_columns = authorities_subjectTerms_df_v0.columns\n",
    "for column in df_columns:\n",
    "    dataType = authorities_subjectTerms_df_v0.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].fillna('null')\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].fillna('null')\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].astype(str)\n",
    "    if dataType == object:\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].fillna('null')\n",
    "        authorities_subjectTerms_df_v0[column] = authorities_subjectTerms_df_v0[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert id to string\n",
    "authorities_subjectTerms_df_v0['001'] = authorities_subjectTerms_df_v0['001'].astype(str)\n",
    "authorities_subj_df_v1 = authorities_subjectTerms_df_v0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "authorities_subj_df_v1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "authorities_subj_df_v1.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of the data\n",
    "authorities_subj_df_v1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test12 = authorities_subj_df_v1[authorities_subj_df_v1['150'].str.contains(\"⑄\", case=False, regex=True)] #\n",
    "query_test12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARILY DROP THE OUTLIER\n",
    "authorities_sub_df_v2 = authorities_subj_df_v1.drop(300).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_sub_df_v2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test13 = authorities_sub_df_v2[authorities_sub_df_v2['150'].str.contains('\"a\":', case=False, regex=True)] #\n",
    "query_test13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_test13.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_sub_df_v2['150a'] = authorities_sub_df_v2['150'].map(lambda x: x.lstrip('\"a\":').rstrip(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authorities_subj_df['150a'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_unique_terms_authority = authorities_sub_df_v2['150a'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(list_unique_terms_authority))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_unique_terms_authority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_sub_df_v2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in list_unique_terms_authority:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_sub_df_v2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_sub_df = authorities_sub_df_v2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorities_sub_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "# Comparing biblio unique terms with authorities unique terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA = biblio_subjectUniqueTerms\n",
    "dfB = authorities_sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfB.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This python script detects the string similarity between two lists of concepts/terms.\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Create an empty DataFrame to store matches\n",
    "mapped_candidates = pd.DataFrame()\n",
    "\n",
    "# Define score thresholds\n",
    "rangeScoreHigh = 100  # Define explicitly\n",
    "rangeScoreMid = 99     # Define if needed\n",
    "\n",
    "# List to accumulate matches (faster than appending to DataFrame directly)\n",
    "row_list = []  \n",
    "\n",
    "############################## CAPTURE VARIABLES FROM DFs #######################################\n",
    "for indexB, rowB in tqdm(dfB.iterrows(), total=dfB.shape[0]):\n",
    "    # Capture basic standard columns for the mapping dataset B (to be mapped) as variables\n",
    "    nameStringB = rowB['150a']  # Access row directly for efficiency\n",
    "    sleep(0.01)  # Optional\n",
    "\n",
    "    for indexA, rowA in dfA.iterrows():\n",
    "        # Capture standard columns for dataset A\n",
    "        nameStringA = rowA['650a']  # Access row directly\n",
    "\n",
    "        ############################## SET STRING MATCHING SETTINGS #######################################\n",
    "        matchScore1 = fuzz.token_sort_ratio(nameStringA, nameStringB)\n",
    "        matchScore2 = fuzz.token_set_ratio(nameStringA, nameStringB)\n",
    "\n",
    "        ############################## RUN STRING MATCHING #######################################\n",
    "        if matchScore1 == rangeScoreHigh:\n",
    "            row_list.append({\n",
    "                'match_nameStringB': nameStringB,\n",
    "                'nameStringA': nameStringA,\n",
    "            })\n",
    "\n",
    "# Convert list to DataFrame once (efficient)\n",
    "if row_list:\n",
    "    mapped_candidates = pd.DataFrame(row_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_candidates.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "# THESAURUS (poolparty)\n",
    "Here I import an export from Poolparty with the concepts and their broader and narrower terms.\n",
    "Here is the query I used:\n",
    "\n",
    "```\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "\n",
    "SELECT ?Concept \n",
    "       (GROUP_CONCAT(DISTINCT ?Label; separator=\", \") AS ?PreferredLabels)\n",
    "       (GROUP_CONCAT(DISTINCT ?AltLabel; separator=\", \") AS ?AlternativeLabels)\n",
    "       (GROUP_CONCAT(DISTINCT CONCAT(STR(?Broader), \" (\", COALESCE(?BroaderLabel, \"No Label\"), \")\"); separator=\", \") AS ?BroaderConcepts)\n",
    "       (GROUP_CONCAT(DISTINCT CONCAT(STR(?Narrower), \" (\", COALESCE(?NarrowerLabel, \"No Label\"), \")\"); separator=\", \") AS ?NarrowerConcepts)\n",
    "WHERE {\n",
    "  ?Concept a skos:Concept .\n",
    "  OPTIONAL { ?Concept skos:prefLabel ?Label . }\n",
    "  OPTIONAL { ?Concept skos:altLabel ?AltLabel . }\n",
    "  \n",
    "  OPTIONAL { \n",
    "    ?Concept skos:broader ?Broader .\n",
    "    OPTIONAL { ?Broader skos:prefLabel ?BroaderLabel . }\n",
    "  }\n",
    "  \n",
    "  OPTIONAL { \n",
    "    ?Concept skos:narrower ?Narrower .\n",
    "    OPTIONAL { ?Narrower skos:prefLabel ?NarrowerLabel . }\n",
    "  }\n",
    "}\n",
    "GROUP BY ?Concept\n",
    "ORDER BY ?Concept\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv as dataframe\n",
    "poolparty_df_v0 = pd.read_csv(f'{data_directory_subjects}/poolparty/iish-poolparty-query-result.tsv', sep=\"\\t\", low_memory=False)\n",
    "# low_memory=False was set after this warning message: \"/var/folders/3y/xbjxw0b94jxg6x2bcbyjsmmcgvnf7q/T/ipykernel_987/2912965462.py:3: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "poolparty_df_v0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "poolparty_df_v0.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert datatypes and fill in empty values\n",
    "df_columns = poolparty_df_v0.columns\n",
    "for column in df_columns:\n",
    "    dataType = poolparty_df_v0.dtypes[column]\n",
    "    if dataType == np.float64:\n",
    "        poolparty_df_v0[column] = poolparty_df_v0[column].fillna('null')\n",
    "        poolparty_df_v0[column] = poolparty_df_v0[column].astype(str)\n",
    "    if dataType == np.int_:\n",
    "        poolparty_df_v0[column] = poolparty_df_v0[column].fillna('null')\n",
    "        poolparty_df_v0[column] = poolparty_df_v0[column].astype(str)\n",
    "    if dataType == object:\n",
    "        poolparty_df_v0[column] = poolparty_df_v0[column].fillna('null')\n",
    "        poolparty_df_v0[column] = poolparty_df_v0[column].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "poolparty_df_v0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "poolparty_df_v1 = poolparty_df_v0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "test10 = poolparty_df_v1[poolparty_df_v1['PreferredLabels'].str.contains(\"strike*|staking*\", case=False, regex=True)]\n",
    "test10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split column \"PreferredLabels\" to get each label in one row\n",
    "# Step 1: Split the column using \"Ω\" as the separator\n",
    "poolparty_df_v1[\"PreferredLabels\"] = poolparty_df_v1[\"PreferredLabels\"].str.split(\", \")\n",
    "\n",
    "# Step 2: Explode the list into multiple rows\n",
    "poolparty_df_v1 = poolparty_df_v1.explode(\"PreferredLabels\", ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "poolparty_df_v1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "test11 = poolparty_df_v1[poolparty_df_v1['PreferredLabels'].str.contains(\"strike*|staking*\", case=False, regex=True)]\n",
    "test11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "poolparty_df = poolparty_df_v1.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "poolparty_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You may want to dowload the table above to an excel file for further inspection:\n",
    "# name_file = 'poolparty_concepts_per_label' # for thesaurus report\n",
    "\n",
    "# # field_subset_df.to_excel(f'{data_downloads}/{name_file}.xlsx')\n",
    "\n",
    "# ## or download to csv\n",
    "# poolparty_df.to_csv(f'{data_directory_subjects}/poolparty/{name_file}.csv', index=False) # if too big, use compression='gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "# MAPPINGS authority - Poolparty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "ListA = authorities_sub_df\n",
    "ListB = poolparty_df\n",
    "\n",
    "<!-- Authorities -->\n",
    "<!-- #   Column  Non-Null Count  Dtype \n",
    "---  ------  --------------  ----- \n",
    " 0   001     982 non-null    object\n",
    " 1   150     982 non-null    object\n",
    " 2   450     982 non-null    object\n",
    " 3   550     982 non-null    object\n",
    " 4   leader  982 non-null    object\n",
    " 5   150a    982 non-null    object -->\n",
    "\n",
    "\n",
    "<!-- Poolparty -->\n",
    "<!-- #   Column             Non-Null Count  Dtype \n",
    "---  ------             --------------  ----- \n",
    " 0   Concept            2213 non-null   object\n",
    " 1   PreferredLabels    2213 non-null   object\n",
    " 2   AlternativeLabels  2213 non-null   object\n",
    " 3   BroaderConcepts    2213 non-null   object\n",
    " 4   NarrowerConcepts   2213 non-null   object -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This python script detects the string similarity between two lists of concepts/terms.\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "\n",
    "def compare_strings(dfA, dfB):\n",
    "    '''Processes and maps candidate names\n",
    "    Inputs are two dataframes of names\n",
    "    Outputs a dataframe of candidates\n",
    "    '''\n",
    "\n",
    "    # # create an empty dataframe\n",
    "    # mapped_candidates_df = pd.DataFrame()\n",
    "    ##COLLECT ROWS\n",
    "    rows = []\n",
    "\n",
    "    ############################## CAPTURE VARIABLES FROM DFs #######################################\n",
    "    # for indexB, rowB in dfB.iterrows():\n",
    "    for indexB, rowB in tqdm(dfB.iterrows(), total=dfB.shape[0]):\n",
    "        # Capture basic standard columns for the mapping dataset B (to be mapped) as variables\n",
    "        idB = dfB.loc[indexB, 'Concept']\n",
    "        stringB = dfB.loc[indexB, 'PreferredLabels']\n",
    "        alternativeLabelsB = dfB.loc[indexB, 'AlternativeLabels']\n",
    "        broaderConceptsB = dfB.loc[indexB, 'BroaderConcepts']\n",
    "        narrowerConceptsB = dfB.loc[indexB, 'NarrowerConcepts']\n",
    "        sleep(0.01)\n",
    "        for indexA, rowA in dfA.iterrows():\n",
    "            # Capture basic standard columns for the mapping dataset A (to be mapped to) as variables\n",
    "            idA = dfA.loc[indexA, '001']\n",
    "            stringA = dfA.loc[indexA, '150a']\n",
    "            marc150 = dfA.loc[indexA, '150']\n",
    "            marc450 = dfA.loc[indexA, '450']\n",
    "            marc550 = dfA.loc[indexA, '550']        \n",
    "            leader = dfA.loc[indexA, 'leader']\n",
    "\n",
    "    ############################## SET STRING MATCHING SETTINGS #######################################\n",
    "\n",
    "            # Algorithm to be used\n",
    "            matchScore1 = fuzz.token_sort_ratio(stringA.lower(), stringB.lower())\n",
    "            # matchScore2 = fuzz.token_set_ratio(stringA, stringB)\n",
    "            # matchScore3 = fuzz.partial_ratio(nameStringA, nameStringB) # USE WITH casesNoisy (edit below) if names in both datasets are very similar. It compares parts of strings, low score is useful to avoid matches like this (('Carlieri Jacopo', 'Jacopo Battieri'))\n",
    "\n",
    "            # String score ranges\n",
    "            rangeScoreVeryLow = 80\n",
    "            rangeScoreLow = 85\n",
    "            rangeScoreMid = 90\n",
    "            rangeScoreHigh = 100\n",
    "\n",
    "\n",
    "    # ############################## RUN STRING MATCHING #######################################\n",
    "            # this rule only applies to cases of type A when the dates are exactly the same (e.g., to match 'Olivarius Vredius' with 'Olivier de Wree')\n",
    "            if rangeScoreVeryLow <= matchScore1 <= rangeScoreHigh:\n",
    "                scoreString = dfA.loc[indexA,'scoreString'] = matchScore1\n",
    "                scoreType = dfA.loc[indexA, 'scoreType'] = 'matchScore1'\n",
    "                match_StringB = dfA.loc[indexA,'match-stringB'] = stringB\n",
    "                match_idB = idB = dfA.loc[indexA,'match-idB'] = idB\n",
    "                rows.append({\n",
    "                    'scoreString': scoreString,\n",
    "                    'scoreType': scoreType,\n",
    "                    'idA':idA,                    \n",
    "                    'match_idB': match_idB,\n",
    "                    'stringA': stringA,                    \n",
    "                    'match_stringB': match_StringB,\n",
    "                    'marc150': marc150, \n",
    "                    'marc450': marc450, \n",
    "                    'marc550': marc550, \n",
    "                    'leader': leader,\n",
    "                    'alternativeLabelsB': alternativeLabelsB, \n",
    "                    'broaderConceptsB': broaderConceptsB, \n",
    "                    'narrowerConceptsB': narrowerConceptsB\n",
    "                })\n",
    "\n",
    "        df_mapped = pd.DataFrame(rows)\n",
    "\n",
    "    return df_mapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfA = authorities_sub_df\n",
    "# dfB = poolparty_df\n",
    "\n",
    "mapped_candidates = compare_strings(authorities_sub_df, poolparty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_candidates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_candidates.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "test12 = mapped_candidates[mapped_candidates['StringA'].str.contains(\"strike*|staking*\", case=False, regex=True)]\n",
    "test12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT USED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how many terms are in Biblio that have a correspondent Authority record?\n",
    "# biblio_subjectTerms_notnull = biblio_subjectTerms_df[biblio_subjectTerms_df['650'] == 'notnull']\n",
    "# question2 = biblio_subjectTerms_notnull[biblio_subjectTerms_notnull['\"0\"'].str.contains(\"NL-AMISG)\", case=True, regex=False)]\n",
    "# question2_total = question2.shape[0]\n",
    "# question2_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # how many terms are in Biblio that do not have a correspondent Authority record?\n",
    "# biblio_subjectTerms_a_notnull = biblio_subjectTerms_df[biblio_subjectTerms_df['\"a\"'] is not ]\n",
    "# question3 = biblio_subjectTerms_notnull[~biblio_subjectTerms_notnull['\"0\"'].str.contains(\"\", case=True, regex=False)]\n",
    "# question3_total = biblio_subjectTerms_null.shape[0]\n",
    "# question3_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# questionTest = biblio_subjectTerms_df['\"0\"'].value_counts()\n",
    "# questionTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_test = subject_terms_df[\n",
    "#     subject_terms_df['\"a\"'].str.contains(\"collective\", case=False, regex=True) & \n",
    "#     subject_terms_df[\"another_column\"].str.contains(\"stringTest\", case=False, regex=True)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_terms_df['\"x\"'].unique()\n",
    "# query_test = subject_terms_df[subject_terms_df['\"a\"'].str.contains(\"collective\", case=False, regex=True)] #\n",
    "# query_test\n",
    "# query_test['\"a\"'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
